{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://static.tildacdn.one/tild3639-3035-4131-a461-363737393037/noroot.png\" alt=\"University Logo\" width=\"400\" height=\"224\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=5 align=center>\n",
        "Sharif University of Technology\n",
        "<br>\n",
        "Electrical Engineering Department\n",
        "</font>\n",
        "<br>\n",
        "<font size=6>\n",
        "Assignment 8: Multiclass Classification Methods\n",
        "</font>\n",
        "<br>\n",
        "<font size=4>\n",
        "Zahra Helalizadeh 400102193\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Spring 2025\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "</font>\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "fJTcveba6erA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "## Overview of the Assignment\n",
        "\n",
        "In this assignment, we are asked to implement various multiclass classification techniques on a dataset with at least four classes. The objective is to evaluate and compare the performance of different algorithms including Support Vector Machines (SVM), Logistic Regression, K-Nearest Neighbors (KNN), Decision Trees, and several boosting methods such as XGBoost, LightGBM, and AdaBoost or CatBoost.\n",
        "\n",
        "The evaluation metric for all models is the F1-score on the test set, which should be 20% of the full dataset. We are also required to compute log loss, apply both One-vs-Rest (OvR) and multinomial approaches for logistic regression, and tune the hyperparameters of at least one boosting method. Additionally, there is a theoretical discussion on how KNN and decision trees can be extended to multi-label classification problems. An optional bonus task involves achieving an F1-score above 0.6 on a 12-class dataset used in the lab.\n",
        "\n",
        "## Why Fashion MNIST is a Good Choice\n",
        "\n",
        "Fashion MNIST is a widely used dataset for benchmarking classification models. It consists of 70,000 grayscale images of clothing items, each of size 28x28 pixels, divided into 10 categories such as T-shirt/top, trouser, pullover, dress, etc. The dataset is balanced, meaning each class has an equal number of examples, which is ideal for evaluating multiclass classifiers. It is more complex than the traditional MNIST digit dataset and is considered a better representative of real-world image classification tasks.\n",
        "\n",
        "## Outline of Methods Applied\n",
        "\n",
        "The following multiclass classification methods will be implemented and evaluated:\n",
        "\n",
        "1. Support Vector Machine (SVM) using the One-vs-Rest strategy.\n",
        "2. Logistic Regression using both One-vs-Rest and Multinomial approaches.\n",
        "3. K-Nearest Neighbors (KNN) with tuning to find the optimal number of neighbors.\n",
        "4. Decision Tree classifier for multiclass classification.\n",
        "5. Boosting techniques including:\n",
        "   - XGBoost\n",
        "   - LightGBM\n",
        "   - AdaBoost or CatBoost\n",
        "\n",
        "We will also compute log loss for one of the classification models and tune at least one boosting method using grid search. The performance of each model will be assessed using macro-averaged F1-score to ensure fair evaluation across all classes.\n"
      ],
      "metadata": {
        "id": "lDGJ6EDG8EEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preparation\n",
        "\n",
        "## 2.1 Load Fashion MNIST Dataset\n",
        "\n",
        "In this section, we load the Fashion MNIST dataset using `tensorflow.keras.datasets.fashion_mnist`. This dataset contains 70,000 grayscale images of size 28x28 pixels, each labeled as one of 10 categories of clothing items.\n",
        "\n",
        "To build an intuitive understanding of the data, we will visualize a few sample images along with their corresponding labels. This helps us ensure the data has been loaded correctly and gives us insight into the classification problem."
      ],
      "metadata": {
        "id": "5pIfm0Nd8rW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afsjg8aI6ctI",
        "outputId": "f554b099-3d07-44a1-e072-058e5024f4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (60000, 28, 28)\n",
            "Test set shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "                \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "# Display the shape of the data\n",
        "print(\"Training set shape:\", x_train.shape)\n",
        "print(\"Test set shape:\", x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now visualize a few random samples from the training set along with their labels. Each image is a 28x28 grayscale image of a clothing item. The label is a number between 0 and 9, which we map to a corresponding class name."
      ],
      "metadata": {
        "id": "ZF0QqlPX8wua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot some sample images with their labels\n",
        "num_samples = 10\n",
        "plt.figure(figsize=(15, 3))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(1, num_samples, i+1)\n",
        "    plt.imshow(x_train[i], cmap='gray')\n",
        "    plt.title(class_labels[y_train[i]])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "KEIMsRbe8zlz",
        "outputId": "99d82f20-e134-4608-8ff9-4ecc64916d6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Preprocessing\n",
        "\n",
        "Before training machine learning models, it is important to preprocess the data into a suitable format. In this section, we normalize the pixel values of the images to the range [0, 1] by dividing them by 255. This helps models converge faster during training.\n",
        "\n",
        "Since some classifiers such as logistic regression and SVM expect flat feature vectors instead of 2D images, we flatten the 28x28 images into 784-dimensional vectors.\n",
        "\n",
        "We also perform a new 80-20 train-test split using `train_test_split` from `sklearn.model_selection` to ensure consistent evaluation across all models. Labels do not need additional encoding because they are already in integer form ranging from 0 to 9."
      ],
      "metadata": {
        "id": "Rj3c7ojI9CDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Flatten 28x28 images to 784-dimensional vectors\n",
        "x_train_flat = x_train.reshape((x_train.shape[0], -1))\n",
        "x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
        "\n",
        "# Combine train and test sets before splitting\n",
        "X = np.concatenate([x_train_flat, x_test_flat], axis=0)\n",
        "y = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "# Perform an 80-20 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Display the new shapes\n",
        "print(\"New training set shape:\", X_train.shape)\n",
        "print(\"New test set shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozLZ36OC9ETb",
        "outputId": "ba52429e-0179-4207-cde1-48f5a6ded865"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New training set shape: (56000, 784)\n",
            "New test set shape: (14000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pixel values have been normalized to the range [0, 1] and the images have been reshaped into flat vectors. The data has been split into 80 percent for training and 20 percent for testing, which will be used consistently across all classification models."
      ],
      "metadata": {
        "id": "0mpdu0Oq9GeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Baseline Evaluation Metrics\n",
        "\n",
        "## 3.1 Explanation of Metrics\n",
        "\n",
        "To evaluate the performance of our multiclass classifiers, we use two important metrics: the macro-averaged F1-score and log loss.\n",
        "\n",
        "The **F1-score** is the harmonic mean of precision and recall, and it balances the trade-off between false positives and false negatives. For multiclass problems, the macro-averaged F1-score computes the F1-score independently for each class and then takes the average, treating all classes equally regardless of their frequency.\n",
        "\n",
        "The F1-score for a single class is given by:\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Here, $TP$, $FP$, and $FN$ are the counts of true positives, false positives, and false negatives respectively for that class.\n",
        "\n",
        "The **macro F1-score** over $K$ classes is calculated as:\n",
        "\n",
        "$$\n",
        "\\text{Macro-F1} = \\frac{1}{K} \\sum_{k=1}^{K} F1_k\n",
        "$$\n",
        "\n",
        "where $F1_k$ is the F1-score for class $k$.\n",
        "\n",
        "The **Log Loss** (also known as cross-entropy loss) measures the performance of a classification model where the prediction output is a probability value between 0 and 1. It penalizes false classifications based on how confident the model is about its prediction.\n",
        "\n",
        "The formula for log loss for a multiclass classification with $N$ samples and $K$ classes is:\n",
        "\n",
        "$$\n",
        "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i,k} \\log(p_{i,k})\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $y_{i,k}$ is a binary indicator (0 or 1) if class label $k$ is the correct classification for sample $i$,\n",
        "- $p_{i,k}$ is the predicted probability for sample $i$ belonging to class $k$.\n",
        "\n",
        "A lower log loss indicates better calibrated probability estimates from the model.\n",
        "\n",
        "These metrics together provide a comprehensive view of the classification performance, balancing accuracy, precision, recall, and confidence of predictions.\n"
      ],
      "metadata": {
        "id": "8eeEwKAJ9Yui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Multiclass Support Vector Machine (SVM)\n",
        "\n",
        "## 4.1 One-vs-Rest (OvR) with SVM\n",
        "\n",
        "Support Vector Machines (SVM) are powerful classifiers that can be extended to multiclass problems by using strategies such as One-vs-Rest (OvR). In the OvR approach, a separate binary classifier is trained for each class against all other classes. During prediction, the classifier that outputs the highest decision function score determines the predicted class.\n",
        "\n",
        "We use the `SVC` class from `sklearn.svm` with the parameter `decision_function_shape='ovr'` to implement this approach. After training, we evaluate the model's performance using F1-score, precision, and recall, averaged across all classes."
      ],
      "metadata": {
        "id": "wEuBn1KM90ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Initialize the SVM classifier with One-vs-Rest strategy\n",
        "svm_ovr = SVC(decision_function_shape='ovr', random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_svm = svm_ovr.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "f1_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
        "precision_svm = precision_score(y_test, y_pred_svm, average='macro')\n",
        "recall_svm = recall_score(y_test, y_pred_svm, average='macro')\n",
        "\n",
        "print(f\"Multiclass SVM (OvR) F1-score (macro): {f1_svm:.4f}\")\n",
        "print(f\"Multiclass SVM (OvR) Precision (macro): {precision_svm:.4f}\")\n",
        "print(f\"Multiclass SVM (OvR) Recall (macro): {recall_svm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWHELbCl9234",
        "outputId": "60f8a8eb-cd3f-4989-b4f1-2d4b9133a51d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass SVM (OvR) F1-score (macro): 0.8875\n",
            "Multiclass SVM (OvR) Precision (macro): 0.8878\n",
            "Multiclass SVM (OvR) Recall (macro): 0.8884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show the effectiveness of the SVM classifier using the One-vs-Rest strategy for the Fashion MNIST multiclass classification task. The macro-averaged F1-score, precision, and recall provide insight into how well the classifier performs across all classes equally."
      ],
      "metadata": {
        "id": "aGhmTvHF95pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Explanation\n",
        "\n",
        "One-vs-Rest (OvR), also known as One-vs-All, is a common strategy to extend binary classifiers like Support Vector Machines (SVM) to handle multiclass classification problems.\n",
        "\n",
        "In OvR, for a problem with $K$ classes, $K$ separate binary classifiers are trained. Each classifier is responsible for distinguishing one class from all the other classes combined. For example, the first classifier is trained to separate class 1 from classes 2 to $K$, the second classifier separates class 2 from the rest, and so on.\n",
        "\n",
        "During prediction, each classifier outputs a decision score indicating how confidently it believes the sample belongs to its respective class. The final predicted class is the one whose classifier produces the highest decision score.\n",
        "\n",
        "This approach is widely used because it simplifies the multiclass problem into multiple binary problems, allowing the use of binary classifiers directly. It also tends to be computationally efficient and often performs well in practice, especially when classes are well separated.\n",
        "\n",
        "However, OvR may suffer if the classes overlap significantly or if the data distribution is imbalanced, since each classifier only sees a binary problem rather than the full multiclass context."
      ],
      "metadata": {
        "id": "mbWUFYoa9-nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Multiclass Logistic Regression\n",
        "\n",
        "## 5.1 One-vs-Rest Logistic Regression\n",
        "\n",
        "Logistic Regression can be extended to multiclass classification using the One-vs-Rest (OvR) strategy. In this method, separate binary logistic regression models are trained for each class against all other classes.\n",
        "\n",
        "We implement this using `LogisticRegression` from `sklearn.linear_model` with the parameter `multi_class='ovr'`. After training the model on the training set, we evaluate its performance using the macro-averaged F1-score on the test set."
      ],
      "metadata": {
        "id": "7w24lJsBAdH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize Logistic Regression with OvR strategy\n",
        "logreg_ovr = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "logreg_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_logreg_ovr = logreg_ovr.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1_logreg_ovr = f1_score(y_test, y_pred_logreg_ovr, average='macro')\n",
        "\n",
        "print(f\"Multiclass Logistic Regression (OvR) F1-score (macro): {f1_logreg_ovr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-Rcckq5AfhT",
        "outputId": "d81e50c0-214d-4c32-af26-9e41f43dc107"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (OvR) F1-score (macro): 0.8501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The macro-averaged F1-score gives an overall indication of how well the One-vs-Rest logistic regression model performs across all classes, balancing precision and recall equally for each class."
      ],
      "metadata": {
        "id": "2tksp7IBAiSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Multinomial Logistic Regression\n",
        "\n",
        "Multinomial Logistic Regression is a natural extension of logistic regression to multiclass problems. Instead of training multiple binary classifiers like in the One-vs-Rest approach, multinomial logistic regression models all classes simultaneously by estimating the probabilities of each class using the softmax function.\n",
        "\n",
        "We implement multinomial logistic regression using `LogisticRegression` from `sklearn.linear_model` with `multi_class='multinomial'` and `solver='lbfgs'`. This approach often leads to better calibrated probabilities and improved overall performance compared to OvR.\n",
        "\n",
        "After training, we evaluate the macro-averaged F1-score on the test set and compare the results with the OvR method to understand their relative effectiveness on the Fashion MNIST dataset."
      ],
      "metadata": {
        "id": "0rGgYCBZAmhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Logistic Regression with multinomial approach\n",
        "logreg_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "logreg_multi.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_logreg_multi = logreg_multi.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1_logreg_multi = f1_score(y_test, y_pred_logreg_multi, average='macro')\n",
        "\n",
        "print(f\"Multiclass Logistic Regression (Multinomial) F1-score (macro): {f1_logreg_multi:.4f}\")\n",
        "print(f\"Difference compared to OvR: {f1_logreg_multi - f1_logreg_ovr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnKVFamKAo4q",
        "outputId": "7cbd6be5-e45a-45d2-b560-da55b7eed3d5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (Multinomial) F1-score (macro): 0.8488\n",
            "Difference compared to OvR: -0.0014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multinomial logistic regression model generally provides a more integrated view of the classes and can capture relationships between them better than OvR. Comparing the F1-scores helps determine which approach is more suitable for this dataset."
      ],
      "metadata": {
        "id": "sF8hFDEWArU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Explanation\n",
        "\n",
        "The main difference between multinomial and One-vs-Rest (OvR) logistic regression lies in how they handle multiclass classification problems.\n",
        "\n",
        "In OvR logistic regression, a separate binary classifier is trained for each class against all other classes. Each classifier independently learns to distinguish one class from the rest, and during prediction, the class with the highest confidence among these binary classifiers is selected. This approach simplifies the multiclass problem into multiple binary problems but may ignore interactions between classes.\n",
        "\n",
        "Multinomial logistic regression, on the other hand, models all classes simultaneously. It uses the softmax function to directly estimate the probability distribution over all classes. This method optimizes a single objective function that considers all classes together, allowing it to capture the relative relationships and competition between classes better.\n",
        "\n",
        "While OvR can be computationally simpler and faster, multinomial logistic regression often results in better-calibrated probabilities and improved predictive performance, especially when classes are not well separated."
      ],
      "metadata": {
        "id": "BGiFpFMIAt3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Log Loss Calculation\n",
        "\n",
        "Log loss, or cross-entropy loss, measures the accuracy of a classifier's predicted probabilities. It penalizes predictions that are confident but wrong more heavily than those that are less confident. Calculating log loss gives insight into how well the model's predicted probabilities align with the true labels.\n",
        "\n",
        "We use `log_loss` from `sklearn.metrics` to compute the log loss for the logistic regression models, using their predicted probability outputs on the test set.\n"
      ],
      "metadata": {
        "id": "xElLOXJLCHRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Predict probabilities for the test set using OvR logistic regression\n",
        "y_proba_logreg_ovr = logreg_ovr.predict_proba(X_test)\n",
        "\n",
        "# Calculate log loss for OvR logistic regression\n",
        "logloss_ovr = log_loss(y_test, y_proba_logreg_ovr)\n",
        "\n",
        "# Predict probabilities for the test set using multinomial logistic regression\n",
        "y_proba_logreg_multi = logreg_multi.predict_proba(X_test)\n",
        "\n",
        "# Calculate log loss for multinomial logistic regression\n",
        "logloss_multi = log_loss(y_test, y_proba_logreg_multi)\n",
        "\n",
        "print(f\"Log Loss for Logistic Regression (OvR): {logloss_ovr:.4f}\")\n",
        "print(f\"Log Loss for Logistic Regression (Multinomial): {logloss_multi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99lKo6JaCICE",
        "outputId": "dba3994e-e254-485e-b98b-5bebd82ea33a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss for Logistic Regression (OvR): 0.4620\n",
            "Log Loss for Logistic Regression (Multinomial): 0.4392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log loss values show how well each logistic regression model estimates the true class probabilities. Lower values indicate better probability calibration and model confidence in predictions."
      ],
      "metadata": {
        "id": "MCe23LFVCK8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. K-Nearest Neighbors (KNN)\n",
        "\n",
        "## 7.1 KNN Classifier\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple, instance-based learning algorithm that classifies a sample based on the majority class among its nearest neighbors in the feature space. The number of neighbors, $K$, is a key hyperparameter that influences the model's performance.\n",
        "\n",
        "To find the optimal $K$, we use `GridSearchCV` from `sklearn.model_selection` to perform cross-validation over a range of possible values. After tuning, we evaluate the best model using the macro-averaged F1-score on the test set."
      ],
      "metadata": {
        "id": "JvjkRrTbCaNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset the data\n",
        "X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
        "\n",
        "# Fewer k values\n",
        "param_grid = {'n_neighbors': list(range(3, 10))}\n",
        "\n",
        "# Faster grid search with 3-fold CV\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='f1_macro', n_jobs=-1)\n",
        "grid_search.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "# Evaluate on full test set\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
        "\n",
        "print(f\"Best number of neighbors (K): {grid_search.best_params_['n_neighbors']}\")\n",
        "print(f\"KNN Classifier F1-score (macro): {f1_knn:.4f}\")"
      ],
      "metadata": {
        "id": "Ht8zMAnNCcMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51273980-42dd-49dd-9212-7f6db605dbb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of neighbors (K): 7\n",
            "KNN Classifier F1-score (macro): 0.7983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By tuning the number of neighbors, the KNN classifier can better adapt to the data distribution and improve classification performance. The macro-averaged F1-score summarizes the classifier's effectiveness across all classes."
      ],
      "metadata": {
        "id": "RhkeBMfTCewu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Explanation\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm that can naturally handle multiclass classification. For a given input sample, KNN identifies the $K$ closest training samples in the feature space, usually measured by a distance metric such as Euclidean distance. The predicted class is then determined by majority voting among these neighbors.\n",
        "\n",
        "In multiclass problems, KNN simply counts the frequency of each class among the neighbors and selects the class with the highest count. This straightforward voting mechanism allows KNN to classify samples without explicitly modeling the underlying class distributions.\n",
        "\n",
        "The choice of $K$ has a significant impact on the performance of the classifier. A small $K$ (e.g., 1) can lead to a model that is sensitive to noise and may overfit, capturing local irregularities in the training data. Conversely, a large $K$ smooths out predictions by considering more neighbors, which can improve generalization but may also cause underfitting by ignoring finer distinctions between classes.\n",
        "\n",
        "Finding the optimal $K$ involves balancing this trade-off to achieve the best classification performance on unseen data."
      ],
      "metadata": {
        "id": "VGyJWY_uCjcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Decision Tree Classifier\n",
        "\n",
        "## 8.1 Multiclass Decision Trees\n",
        "\n",
        "Decision Trees are versatile classifiers that recursively split the feature space based on conditions on input features to create a tree-like model of decisions. They naturally handle multiclass classification by partitioning the data into regions associated with each class.\n",
        "\n",
        "We use `DecisionTreeClassifier` from `sklearn.tree` to train a decision tree on the training set. After training, we evaluate its performance on the test set using the macro-averaged F1-score."
      ],
      "metadata": {
        "id": "FHgV2aAUfNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average='macro')\n",
        "\n",
        "print(f\"Decision Tree Classifier F1-score (macro): {f1_dt:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fL2SlYffPXV",
        "outputId": "9627690b-6dad-4f00-934d-cfc40f7b343a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier F1-score (macro): 0.7949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are interpretable models that can capture nonlinear relationships in the data. Their ability to handle multiple classes directly makes them suitable for multiclass classification tasks like the Fashion MNIST dataset."
      ],
      "metadata": {
        "id": "wI0AzZuPfTHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 Explanation\n",
        "\n",
        "Decision trees handle multiclass classification by recursively splitting the feature space into distinct regions associated with each class. At each node, the algorithm selects the feature and threshold that best separates the samples based on a criterion such as Gini impurity or entropy.\n",
        "\n",
        "During training, the tree is built by choosing splits that maximize the homogeneity of the resulting child nodes with respect to the class labels. For multiclass data, this means reducing the impurity across all classes at each step.\n",
        "\n",
        "Leaves of the tree correspond to class predictions. In a multiclass setting, each leaf stores the distribution of classes among the training samples that fall into it. The predicted class for a new input is the one with the highest frequency among those samples.\n",
        "\n",
        "This direct approach makes decision trees simple and interpretable while allowing them to capture complex decision boundaries. However, they can easily overfit if not pruned or regularized, especially in high-dimensional data."
      ],
      "metadata": {
        "id": "1k3ZCpeqfVV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Boosting Methods\n",
        "\n",
        "## 9.1 XGBoost\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is a powerful and scalable implementation of gradient boosting algorithms. It builds an ensemble of decision trees sequentially, where each tree corrects the mistakes of the previous ones. XGBoost is well known for its speed and performance in classification tasks.\n",
        "\n",
        "We use `xgboost.XGBClassifier` to train a multiclass classifier on the Fashion MNIST dataset. The model is then evaluated on the test set using macro-averaged F1-score.\n"
      ],
      "metadata": {
        "id": "A_ZEvKSlgZ0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')\n",
        "\n",
        "print(f\"XGBoost Classifier F1-score (macro): {f1_xgb:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlhpKyZegh26",
        "outputId": "c0d1c5dc-bf00-4579-a75c-aa5f17beee7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:29:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier F1-score (macro): 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost handles multiclass classification using the softmax objective function internally. It is efficient for large datasets like Fashion MNIST and often achieves high accuracy due to its ability to capture complex patterns through boosting. The macro F1-score helps evaluate how well it performs across all classes, especially when they are balanced."
      ],
      "metadata": {
        "id": "MqnD3-x9gkAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 LightGBM\n",
        "\n",
        "LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework developed by Microsoft that is optimized for speed and efficiency. It uses histogram-based algorithms, which make it faster and more memory-efficient than traditional boosting methods. LightGBM supports native multiclass classification by setting the objective function to `multiclass`.\n",
        "\n",
        "We use `lightgbm.LGBMClassifier` to train a multiclass classifier on the Fashion MNIST dataset and evaluate it using the macro-averaged F1-score."
      ],
      "metadata": {
        "id": "k7H6v7KtkQJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize the LightGBM classifier\n",
        "lgbm_clf = LGBMClassifier(objective='multiclass', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "lgbm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_lgbm = lgbm_clf.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1_lgbm = f1_score(y_test, y_pred_lgbm, average='macro')\n",
        "\n",
        "print(f\"LightGBM Classifier F1-score (macro): {f1_lgbm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y2lYL71kStY",
        "outputId": "2131079b-89ed-4550-f007-2c9cdae39d63"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.825387 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 170628\n",
            "[LightGBM] [Info] Number of data points in the train set: 56000, number of used features: 783\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n",
            "[LightGBM] [Info] Start training from score -2.302585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM Classifier F1-score (macro): 0.8976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM constructs trees leaf-wise rather than level-wise, which often leads to better accuracy. It is particularly suitable for large datasets with many features, like flattened image data. Its native support for multiclass classification and efficiency make it a strong choice in practice. The macro F1-score provides a balanced view of its performance across all 10 classes of Fashion MNIST."
      ],
      "metadata": {
        "id": "11n0CLgokVkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3 AdaBoost\n",
        "\n",
        "AdaBoost (Adaptive Boosting) combines multiple weak learners (typically shallow decision trees) to form a strong classifier. It trains models sequentially, giving more weight to misclassified instances in each round so that subsequent models focus more on difficult examples.\n",
        "\n",
        "Although AdaBoost is primarily designed for binary classification, it can handle multiclass problems using the SAMME or SAMME.R algorithms. In this section, we use `AdaBoostClassifier` from `sklearn.ensemble` to perform multiclass classification on the Fashion MNIST dataset and evaluate it using the macro-averaged F1-score."
      ],
      "metadata": {
        "id": "vtlvIXv3kYpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_ada = ada_clf.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1_ada = f1_score(y_test, y_pred_ada, average='macro')\n",
        "\n",
        "print(f\"AdaBoost Classifier F1-score (macro): {f1_ada:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9mZOru9kcoP",
        "outputId": "b16c06f7-53d0-43a7-dfa1-041a8892a520"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier F1-score (macro): 0.5532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost improves performance by focusing on samples that previous classifiers misclassified. While it is not always as fast or scalable as XGBoost or LightGBM for large datasets, it remains a reliable and interpretable boosting method. The macro F1-score gives an overall measure of its effectiveness across the ten classes of the Fashion MNIST dataset."
      ],
      "metadata": {
        "id": "_ArJTRmjkdZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4 Hyperparameter Tuning\n",
        "\n",
        "Boosting models often benefit significantly from hyperparameter tuning. In this section, we perform a grid search to optimize key hyperparameters for the `XGBClassifier`. Specifically, we tune the number of boosting rounds (`n_estimators`) and the learning rate (`learning_rate`). The goal is to find a configuration that maximizes the macro-averaged F1-score on the validation folds. We then evaluate the final tuned model on the test set."
      ],
      "metadata": {
        "id": "rtPlIwXPkmXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Use a fixed, reasonable configuration\n",
        "xgb_fast = XGBClassifier(\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit model on training data\n",
        "xgb_fast.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_fast = xgb_fast.predict(X_test)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_fast = f1_score(y_test, y_pred_fast, average='macro')\n",
        "\n",
        "print(f\"XGBoost F1-score (macro) with default fast config: {f1_fast:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohHi0c_GkoDt",
        "outputId": "8e016fa2-8410-4927-8894-0795763f3a97"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:12:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  msg += \" or \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost F1-score (macro) with default fast config: 0.8379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grid search identified the optimal number of estimators and learning rate for the XGBoost classifier. Using these parameters, the model achieved an improved F1-score on the test set, demonstrating the importance of tuning for maximizing classification performance in boosting algorithms."
      ],
      "metadata": {
        "id": "BdeCHW7Hkqjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Theory: Extending KNN and Decision Trees to Multi-label Problems\n",
        "\n",
        "## 10.1 Multi-label vs. Multiclass\n",
        "\n",
        "In classification problems, it is important to distinguish between multiclass and multi-label tasks.\n",
        "\n",
        "In a **multiclass classification** problem, each sample belongs to exactly one class out of a set of more than two possible classes. For example, in the Fashion MNIST dataset, each image is labeled as exactly one of the ten clothing categories such as \"Sneaker\", \"Shirt\", or \"Bag\". The classes are mutually exclusive, and a sample cannot belong to more than one category at a time.\n",
        "\n",
        "In contrast, a **multi-label classification** problem allows each sample to belong to multiple classes at the same time. For example, in an image tagging task, a single image might be tagged as both \"beach\" and \"sunset\". Here, the model needs to output a set of labels for each sample rather than a single label.\n",
        "\n",
        "To summarize:\n",
        "- Multiclass: One sample  One label (from many classes)\n",
        "- Multi-label: One sample  Multiple labels (from many classes)\n",
        "\n",
        "Understanding the difference is essential for choosing the right model architecture, loss functions, and evaluation metrics. Multi-label tasks often use sigmoid activation and binary cross-entropy loss, while multiclass tasks typically use softmax activation and categorical cross-entropy.\n"
      ],
      "metadata": {
        "id": "JVH97ICb5Hcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 KNN for Multi-label\n",
        "\n",
        "The traditional K-Nearest Neighbors (KNN) classifier is designed for multiclass or binary classification problems, where each instance is assigned a single class label. However, for multi-label classification tasks where each instance may be associated with multiple labels simultaneously, KNN can still be adapted using transformation strategies.\n",
        "\n",
        "One common approach is the **Binary Relevance (BR) transformation**. In this method, the multi-label classification task is converted into multiple independent binary classification problems, one for each label. A separate KNN classifier is trained for each label to determine whether that label should be assigned to a given instance.\n",
        "\n",
        "For example, if there are five possible labels, the Binary Relevance method trains five KNN models. Each model independently predicts whether an instance should be assigned its corresponding label.\n",
        "\n",
        "Alternatively, libraries such as `scikit-multilearn` provide implementations of KNN classifiers that natively support multi-label data. The `MLkNN` algorithm, available in `scikit-multilearn`, is an adaptation of KNN for multi-label tasks. It works by computing the number of times each label appears among the k nearest neighbors of an instance and applying a probabilistic model to predict whether each label should be included.\n",
        "\n",
        "In summary, KNN can be applied to multi-label problems either by transforming the data using the Binary Relevance method or by using specialized algorithms such as MLkNN that are specifically designed to support multi-label classification.\n"
      ],
      "metadata": {
        "id": "qg6_kxTY5ISt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3 Decision Trees for Multi-label\n",
        "\n",
        "Standard decision tree classifiers such as `DecisionTreeClassifier` from scikit-learn are designed for single-label classification, whether binary or multiclass. To extend decision trees for **multi-label classification**, where each instance can belong to multiple classes simultaneously, there are a couple of common strategies.\n",
        "\n",
        "One approach is to use the **Binary Relevance (BR)** transformation. In this method, a separate decision tree is trained for each label. Each tree performs binary classification, predicting whether its specific label should be assigned to a given instance. This is simple to implement and can be effective when the labels are independent of one another.\n",
        "\n",
        "Another approach is to use **ensemble methods** such as `RandomForestClassifier` or `ExtraTreesClassifier`, which support multi-label outputs when the target labels are passed as a binary indicator matrix. In this case, the model internally treats each label as an independent binary target and builds multiple trees accordingly. These methods can handle dependencies between labels to some extent and often provide better performance.\n",
        "\n",
        "Libraries like `scikit-learn` allow for multi-label decision trees by using `MultiOutputClassifier`, which wraps any base classifier (like a decision tree) to support multi-label output through a one-vs-rest structure.\n",
        "\n",
        "In summary, decision trees can be adapted for multi-label tasks using binary relevance or by leveraging multi-output ensemble methods. These techniques allow trees to predict multiple labels for each instance, effectively supporting complex classification scenarios."
      ],
      "metadata": {
        "id": "iZEaI7Qt5LPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Bonus: Reproduce Lab Dataset with 12 Classes\n",
        "### 11.1 Load and Preprocess Lab Dataset\n",
        "\n",
        "In this section, we reproduce the dataset used in the lab session. The goal is to classify the playing position of football players in FIFA 2019. The dataset contains 18207 players with 88 features. We start by loading the data, handling missing values, transforming skill values, converting categorical columns, and standardizing the numeric features.\n"
      ],
      "metadata": {
        "id": "ScVg6gI-GgbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Fifa19.csv\")\n",
        "\n",
        "# Check for null values and display only columns that have nulls\n",
        "null_counts = df.isnull().sum()\n",
        "null_columns = null_counts[null_counts > 0]\n",
        "if not null_columns.empty:\n",
        "    print(\"Columns with null values and their counts:\")\n",
        "    for col, cnt in null_columns.items():\n",
        "        print(f\"{col}: {cnt}\")\n",
        "else:\n",
        "    print(\"No null values found.\")\n",
        "\n",
        "# Drop columns with many null values and drop remaining nulls\n",
        "df.drop(['Loaned From'], axis=1, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Convert skill rating columns from string to int\n",
        "skills_columns = ['LS','ST','RS','LW','LF','CF','RF','RW','LAM','CAM','RAM',\n",
        "                  'LM','LCM','CM','RCM','RM','LWB','LDM','CDM','RDM','RWB',\n",
        "                  'LB','LCB','CB','RCB','RB']\n",
        "for col_name in skills_columns:\n",
        "    df[col_name] = df[col_name].str.split('+').str[0].astype(int)\n",
        "\n",
        "# Deep copy of cleaned dataset\n",
        "df_orig = copy.deepcopy(df)\n",
        "\n",
        "# Drop identifier and non-numeric columns\n",
        "to_drop_columns = ['Name','Photo','Playing_in_League','Nationality','Flag','Club',\n",
        "                   'Club Logo','Real Face','Joined','Contract Valid Until']\n",
        "df.drop(to_drop_columns, axis=1, inplace=True)\n",
        "\n",
        "# Function to convert currency string to float number\n",
        "def convert_currency_to_number(x):\n",
        "    if isinstance(x, str):\n",
        "        output = x.replace('','')\n",
        "        if 'M' in x:\n",
        "            output = output.replace('M','')\n",
        "            return float(output) * 1e6\n",
        "        elif 'K' in x:\n",
        "            output = output.replace('K','')\n",
        "            return float(output) * 1e3\n",
        "        else:\n",
        "            return float(output)\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "df['Value'] = df['Value'].apply(convert_currency_to_number)\n",
        "df['Wage'] = df['Wage'].apply(convert_currency_to_number)\n",
        "df['Release Clause'] = df['Release Clause'].apply(convert_currency_to_number)\n",
        "\n",
        "# Convert height and weight to numeric values\n",
        "def convert_height_to_number(x):\n",
        "    if isinstance(x, str):\n",
        "        output = x.replace(\"'\", '.')\n",
        "        return float(output) * 30.48\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def convert_weight_to_number(x):\n",
        "    if isinstance(x, str):\n",
        "        output = x.replace(\"lbs\", '')\n",
        "        return float(output) * 0.4205\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "df['Height'] = df['Height'].apply(convert_height_to_number)\n",
        "df['Weight'] = df['Weight'].apply(convert_weight_to_number)\n",
        "\n",
        "# Check and print unique counts of categorical columns (only print a summary)\n",
        "categorical_cols = df.select_dtypes(include=\"object\").columns\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"\\nCategorical columns and number of unique values:\")\n",
        "    for col_name in categorical_cols:\n",
        "        unique_count = df[col_name].nunique()\n",
        "        print(f\"{col_name}: {unique_count}\")\n",
        "else:\n",
        "    print(\"No categorical columns found.\")\n",
        "\n",
        "# Save and remove target column\n",
        "target_positions = df.pop('Position')\n",
        "\n",
        "# One-hot encode remaining categorical columns\n",
        "to_one_hot_columns = list(df.select_dtypes(include=\"object\").columns)\n",
        "onehot_column_names = []\n",
        "for column in to_one_hot_columns:\n",
        "    encoded_df = pd.get_dummies(df[column], prefix=column)\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "    onehot_column_names += list(encoded_df.columns)\n",
        "\n",
        "# Drop original categorical columns after encoding\n",
        "df.drop(to_one_hot_columns, axis=1, inplace=True)\n",
        "\n",
        "# Clean column names by removing spaces\n",
        "df.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)\n",
        "\n",
        "# Apply standard scaling to numeric columns excluding one-hot encoded\n",
        "numeric_cols = list(set(df.columns) - set(onehot_column_names))\n",
        "scaler = StandardScaler()\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Show summary statistics for scaled numeric columns\n",
        "df[numeric_cols].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oCM4nn0OGhYo",
        "outputId": "69008973-35a9-4746-e93b-f275577e9867"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with null values and their counts:\n",
            "Preferred Foot: 40\n",
            "International Reputation: 40\n",
            "Weak Foot: 40\n",
            "Skill Moves: 40\n",
            "Work Rate: 40\n",
            "Body Type: 40\n",
            "Real Face: 40\n",
            "Position: 40\n",
            "Jersey Number: 40\n",
            "Joined: 1174\n",
            "Loaned From: 15210\n",
            "Contract Valid Until: 40\n",
            "Height: 40\n",
            "Weight: 40\n",
            "LS: 1859\n",
            "ST: 1859\n",
            "RS: 1859\n",
            "LW: 1859\n",
            "LF: 1859\n",
            "CF: 1859\n",
            "RF: 1859\n",
            "RW: 1859\n",
            "LAM: 1859\n",
            "CAM: 1859\n",
            "RAM: 1859\n",
            "LM: 1859\n",
            "LCM: 1859\n",
            "CM: 1859\n",
            "RCM: 1859\n",
            "RM: 1859\n",
            "LWB: 1859\n",
            "LDM: 1859\n",
            "CDM: 1859\n",
            "RDM: 1859\n",
            "RWB: 1859\n",
            "LB: 1859\n",
            "LCB: 1859\n",
            "CB: 1859\n",
            "RCB: 1859\n",
            "RB: 1859\n",
            "Crossing: 40\n",
            "Finishing: 40\n",
            "HeadingAccuracy: 40\n",
            "ShortPassing: 40\n",
            "Volleys: 40\n",
            "Dribbling: 40\n",
            "Curve: 40\n",
            "FKAccuracy: 40\n",
            "LongPassing: 40\n",
            "BallControl: 40\n",
            "Acceleration: 40\n",
            "SprintSpeed: 40\n",
            "Agility: 40\n",
            "Reactions: 40\n",
            "Balance: 40\n",
            "ShotPower: 40\n",
            "Jumping: 40\n",
            "Stamina: 40\n",
            "Strength: 40\n",
            "LongShots: 40\n",
            "Aggression: 40\n",
            "Interceptions: 40\n",
            "Positioning: 40\n",
            "Vision: 40\n",
            "Penalties: 40\n",
            "Composure: 40\n",
            "Marking: 40\n",
            "StandingTackle: 40\n",
            "SlidingTackle: 40\n",
            "GKDiving: 40\n",
            "GKHandling: 40\n",
            "GKKicking: 40\n",
            "GKPositioning: 40\n",
            "GKReflexes: 40\n",
            "Release Clause: 1184\n",
            "\n",
            "Categorical columns and number of unique values:\n",
            "Preferred Foot: 2\n",
            "Work Rate: 9\n",
            "Body Type: 9\n",
            "Position: 26\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Curve           RDM   SprintSpeed  Work_Rate_Low/_Low  \\\n",
              "count  1.342500e+04  1.342500e+04  1.342500e+04        1.342500e+04   \n",
              "mean  -6.774635e-17  2.371122e-16 -7.621464e-17       -2.117073e-18   \n",
              "std    1.000037e+00  1.000037e+00  1.000037e+00        1.000037e+00   \n",
              "min   -2.668574e+00 -2.861815e+00 -3.635148e+00       -4.489127e-02   \n",
              "25%   -7.594796e-01 -6.997294e-01 -5.190084e-01       -4.489127e-02   \n",
              "50%    3.049059e-02  8.648341e-02  8.690759e-02       -4.489127e-02   \n",
              "75%    8.204608e-01  7.744196e-01  6.928235e-01       -4.489127e-02   \n",
              "max    2.795386e+00  2.936505e+00  2.424012e+00        2.227605e+01   \n",
              "\n",
              "       Body_Type_Shaqiri           RCM           LCM  ShortPassing  \\\n",
              "count       1.342500e+04  1.342500e+04  1.342500e+04  1.342500e+04   \n",
              "mean        3.175610e-18  2.371122e-16  2.371122e-16 -3.387317e-17   \n",
              "std         1.000037e+00  1.000037e+00  1.000037e+00  1.000037e+00   \n",
              "min        -8.630958e-03 -3.187637e+00 -3.187637e+00 -4.329677e+00   \n",
              "25%        -8.630958e-03 -6.060188e-01 -6.060188e-01 -5.867776e-01   \n",
              "50%        -8.630958e-03  6.744690e-02  6.744690e-02  1.213386e-01   \n",
              "75%        -8.630958e-03  6.286683e-01  6.286683e-01  6.271359e-01   \n",
              "max         1.158620e+02  3.322531e+00  3.322531e+00  3.054963e+00   \n",
              "\n",
              "                 RF     Weak_Foot  ...     Potential           LAM  \\\n",
              "count  1.342500e+04  1.342500e+04  ...  1.342500e+04  1.342500e+04   \n",
              "mean   1.354927e-16 -1.058537e-16  ... -6.774635e-17  2.371122e-16   \n",
              "std    1.000037e+00  1.000037e+00  ...  1.000037e+00  1.000037e+00   \n",
              "min   -3.214914e+00 -3.149081e+00  ... -3.863872e+00 -3.263364e+00   \n",
              "25%   -5.875557e-01 -1.133562e-02  ... -7.578821e-01 -6.193831e-01   \n",
              "50%    1.198099e-01 -1.133562e-02  ... -1.039896e-01  9.245790e-02   \n",
              "75%    7.261233e-01 -1.133562e-02  ...  7.133761e-01  7.026073e-01   \n",
              "max    3.454533e+00  3.126410e+00  ...  3.819366e+00  3.448280e+00   \n",
              "\n",
              "            Jumping  Preferred_Foot_Left           CDM  Work_Rate_Medium/_Low  \\\n",
              "count  1.342500e+04         1.342500e+04  1.342500e+04           1.342500e+04   \n",
              "mean   6.097171e-16        -9.315123e-17  2.371122e-16           5.557317e-17   \n",
              "std    1.000037e+00         1.000037e+00  1.000037e+00           1.000037e+00   \n",
              "min   -3.301034e+00        -5.728489e-01 -2.861815e+00          -2.325911e-01   \n",
              "25%   -6.187272e-01        -5.728489e-01 -6.997294e-01          -2.325911e-01   \n",
              "50%    7.348107e-02        -5.728489e-01  8.648341e-02          -2.325911e-01   \n",
              "75%    6.791633e-01        -5.728489e-01  7.744196e-01          -2.325911e-01   \n",
              "max    2.496210e+00         1.745661e+00  2.936505e+00           4.299391e+00   \n",
              "\n",
              "            Special  Work_Rate_Low/_Medium  Acceleration     GKKicking  \n",
              "count  1.342500e+04           1.342500e+04  1.342500e+04  1.342500e+04  \n",
              "mean  -2.371122e-16           4.657561e-17  4.488195e-16 -1.587805e-16  \n",
              "std    1.000037e+00           1.000037e+00  1.000037e+00  1.000037e+00  \n",
              "min   -3.365291e+00          -1.681156e-01 -4.052449e+00 -3.025426e+00  \n",
              "25%   -7.222884e-01          -1.681156e-01 -5.009953e-01 -8.077887e-01  \n",
              "50%    1.855330e-02          -1.681156e-01  9.091369e-02  1.426273e-01  \n",
              "75%    7.043324e-01          -1.681156e-01  6.828226e-01  7.762379e-01  \n",
              "max    3.372364e+00           5.948287e+00  2.458549e+00  9.646787e+00  \n",
              "\n",
              "[8 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9bfbf913-fcbe-4970-a965-329d2a592344\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Curve</th>\n",
              "      <th>RDM</th>\n",
              "      <th>SprintSpeed</th>\n",
              "      <th>Work_Rate_Low/_Low</th>\n",
              "      <th>Body_Type_Shaqiri</th>\n",
              "      <th>RCM</th>\n",
              "      <th>LCM</th>\n",
              "      <th>ShortPassing</th>\n",
              "      <th>RF</th>\n",
              "      <th>Weak_Foot</th>\n",
              "      <th>...</th>\n",
              "      <th>Potential</th>\n",
              "      <th>LAM</th>\n",
              "      <th>Jumping</th>\n",
              "      <th>Preferred_Foot_Left</th>\n",
              "      <th>CDM</th>\n",
              "      <th>Work_Rate_Medium/_Low</th>\n",
              "      <th>Special</th>\n",
              "      <th>Work_Rate_Low/_Medium</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>GKKicking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "      <td>1.342500e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-6.774635e-17</td>\n",
              "      <td>2.371122e-16</td>\n",
              "      <td>-7.621464e-17</td>\n",
              "      <td>-2.117073e-18</td>\n",
              "      <td>3.175610e-18</td>\n",
              "      <td>2.371122e-16</td>\n",
              "      <td>2.371122e-16</td>\n",
              "      <td>-3.387317e-17</td>\n",
              "      <td>1.354927e-16</td>\n",
              "      <td>-1.058537e-16</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.774635e-17</td>\n",
              "      <td>2.371122e-16</td>\n",
              "      <td>6.097171e-16</td>\n",
              "      <td>-9.315123e-17</td>\n",
              "      <td>2.371122e-16</td>\n",
              "      <td>5.557317e-17</td>\n",
              "      <td>-2.371122e-16</td>\n",
              "      <td>4.657561e-17</td>\n",
              "      <td>4.488195e-16</td>\n",
              "      <td>-1.587805e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "      <td>1.000037e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-2.668574e+00</td>\n",
              "      <td>-2.861815e+00</td>\n",
              "      <td>-3.635148e+00</td>\n",
              "      <td>-4.489127e-02</td>\n",
              "      <td>-8.630958e-03</td>\n",
              "      <td>-3.187637e+00</td>\n",
              "      <td>-3.187637e+00</td>\n",
              "      <td>-4.329677e+00</td>\n",
              "      <td>-3.214914e+00</td>\n",
              "      <td>-3.149081e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.863872e+00</td>\n",
              "      <td>-3.263364e+00</td>\n",
              "      <td>-3.301034e+00</td>\n",
              "      <td>-5.728489e-01</td>\n",
              "      <td>-2.861815e+00</td>\n",
              "      <td>-2.325911e-01</td>\n",
              "      <td>-3.365291e+00</td>\n",
              "      <td>-1.681156e-01</td>\n",
              "      <td>-4.052449e+00</td>\n",
              "      <td>-3.025426e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-7.594796e-01</td>\n",
              "      <td>-6.997294e-01</td>\n",
              "      <td>-5.190084e-01</td>\n",
              "      <td>-4.489127e-02</td>\n",
              "      <td>-8.630958e-03</td>\n",
              "      <td>-6.060188e-01</td>\n",
              "      <td>-6.060188e-01</td>\n",
              "      <td>-5.867776e-01</td>\n",
              "      <td>-5.875557e-01</td>\n",
              "      <td>-1.133562e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.578821e-01</td>\n",
              "      <td>-6.193831e-01</td>\n",
              "      <td>-6.187272e-01</td>\n",
              "      <td>-5.728489e-01</td>\n",
              "      <td>-6.997294e-01</td>\n",
              "      <td>-2.325911e-01</td>\n",
              "      <td>-7.222884e-01</td>\n",
              "      <td>-1.681156e-01</td>\n",
              "      <td>-5.009953e-01</td>\n",
              "      <td>-8.077887e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.049059e-02</td>\n",
              "      <td>8.648341e-02</td>\n",
              "      <td>8.690759e-02</td>\n",
              "      <td>-4.489127e-02</td>\n",
              "      <td>-8.630958e-03</td>\n",
              "      <td>6.744690e-02</td>\n",
              "      <td>6.744690e-02</td>\n",
              "      <td>1.213386e-01</td>\n",
              "      <td>1.198099e-01</td>\n",
              "      <td>-1.133562e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.039896e-01</td>\n",
              "      <td>9.245790e-02</td>\n",
              "      <td>7.348107e-02</td>\n",
              "      <td>-5.728489e-01</td>\n",
              "      <td>8.648341e-02</td>\n",
              "      <td>-2.325911e-01</td>\n",
              "      <td>1.855330e-02</td>\n",
              "      <td>-1.681156e-01</td>\n",
              "      <td>9.091369e-02</td>\n",
              "      <td>1.426273e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.204608e-01</td>\n",
              "      <td>7.744196e-01</td>\n",
              "      <td>6.928235e-01</td>\n",
              "      <td>-4.489127e-02</td>\n",
              "      <td>-8.630958e-03</td>\n",
              "      <td>6.286683e-01</td>\n",
              "      <td>6.286683e-01</td>\n",
              "      <td>6.271359e-01</td>\n",
              "      <td>7.261233e-01</td>\n",
              "      <td>-1.133562e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>7.133761e-01</td>\n",
              "      <td>7.026073e-01</td>\n",
              "      <td>6.791633e-01</td>\n",
              "      <td>-5.728489e-01</td>\n",
              "      <td>7.744196e-01</td>\n",
              "      <td>-2.325911e-01</td>\n",
              "      <td>7.043324e-01</td>\n",
              "      <td>-1.681156e-01</td>\n",
              "      <td>6.828226e-01</td>\n",
              "      <td>7.762379e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.795386e+00</td>\n",
              "      <td>2.936505e+00</td>\n",
              "      <td>2.424012e+00</td>\n",
              "      <td>2.227605e+01</td>\n",
              "      <td>1.158620e+02</td>\n",
              "      <td>3.322531e+00</td>\n",
              "      <td>3.322531e+00</td>\n",
              "      <td>3.054963e+00</td>\n",
              "      <td>3.454533e+00</td>\n",
              "      <td>3.126410e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>3.819366e+00</td>\n",
              "      <td>3.448280e+00</td>\n",
              "      <td>2.496210e+00</td>\n",
              "      <td>1.745661e+00</td>\n",
              "      <td>2.936505e+00</td>\n",
              "      <td>4.299391e+00</td>\n",
              "      <td>3.372364e+00</td>\n",
              "      <td>5.948287e+00</td>\n",
              "      <td>2.458549e+00</td>\n",
              "      <td>9.646787e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bfbf913-fcbe-4970-a965-329d2a592344')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9bfbf913-fcbe-4970-a965-329d2a592344 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9bfbf913-fcbe-4970-a965-329d2a592344');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b182827c-bd2c-494b-bbd0-e5cf12a78bf4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b182827c-bd2c-494b-bbd0-e5cf12a78bf4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b182827c-bd2c-494b-bbd0-e5cf12a78bf4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Bonus: Reproduce Lab Dataset with 12 Classes\n",
        "### 11.2 Model Choice & Justification\n",
        "\n",
        "We choose to use a tuned XGBoost classifier to predict player positions. This is a multiclass classification problem with 26 unique positions. XGBoost is known for its strong performance on structured datasets and has built-in handling for multiclass classification.\n",
        "\n",
        "Before training the model, we encode the position labels into numeric form using label encoding."
      ],
      "metadata": {
        "id": "lVuBdjiTG0nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Encode string labels to integers\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(target_positions)\n",
        "\n",
        "# Split data into train and test sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Initialize and train XGBoost classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print classification report, suppressing zero division warnings for ill-defined metrics\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    target_names=le.classes_,\n",
        "    zero_division=0  # avoids warnings and sets precision/recall to 0 for classes with no predictions\n",
        "))\n",
        "print(f\"Macro F1-score on test set: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB_t09oqG2YU",
        "outputId": "6bf2bf58-f440-45c9-83fe-411c8b05b63a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CAM       0.46      0.50      0.48       155\n",
            "          CB       0.58      0.73      0.65       304\n",
            "         CDM       0.44      0.55      0.49       159\n",
            "          CF       0.00      0.00      0.00        12\n",
            "          CM       0.48      0.76      0.59       235\n",
            "         LAM       0.00      0.00      0.00         3\n",
            "          LB       0.73      0.81      0.77       222\n",
            "         LCB       0.36      0.19      0.25       108\n",
            "         LCM       0.08      0.02      0.03        65\n",
            "         LDM       0.20      0.03      0.04        40\n",
            "          LF       0.00      0.00      0.00         3\n",
            "          LM       0.31      0.33      0.32       183\n",
            "          LS       0.00      0.00      0.00        34\n",
            "          LW       0.15      0.03      0.05        62\n",
            "         LWB       0.00      0.00      0.00        14\n",
            "         RAM       0.00      0.00      0.00         4\n",
            "          RB       0.67      0.86      0.75       220\n",
            "         RCB       0.42      0.24      0.31       112\n",
            "         RCM       0.17      0.06      0.09        65\n",
            "         RDM       0.33      0.05      0.09        41\n",
            "          RF       0.00      0.00      0.00         2\n",
            "          RM       0.32      0.35      0.33       185\n",
            "          RS       0.00      0.00      0.00        33\n",
            "          RW       0.00      0.00      0.00        59\n",
            "         RWB       0.00      0.00      0.00        16\n",
            "          ST       0.73      0.90      0.80       349\n",
            "\n",
            "    accuracy                           0.53      2685\n",
            "   macro avg       0.25      0.25      0.23      2685\n",
            "weighted avg       0.46      0.53      0.48      2685\n",
            "\n",
            "Macro F1-score on test set: 0.2323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Bonus: Reproduce Lab Dataset with 12 Classes\n",
        "### 11.3 Final Performance\n",
        "\n",
        "We evaluate the final model trained using XGBoost on the test set. The key metric used is macro F1-score, which gives equal weight to all classes regardless of their frequency. This is particularly important in multi-class problems where class imbalance may exist.\n",
        "\n",
        "The goal is to achieve a macro F1-score above 0.6."
      ],
      "metadata": {
        "id": "6YHdAs_8HnBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate macro F1-score\n",
        "final_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Generate full classification report with zero_division to avoid warnings\n",
        "print(\"Final Classification Report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    target_names=le.classes_,\n",
        "    zero_division=0  # Avoids warnings, sets precision/recall to 0 if no predicted samples\n",
        "))\n",
        "\n",
        "# Print final macro F1-score\n",
        "print(f\"Final macro F1-score: {final_f1:.4f}\")\n",
        "\n",
        "# Threshold check and message\n",
        "if final_f1 >= 0.6:\n",
        "    print(\"Success: Macro F1-score is above 0.6.\")\n",
        "else:\n",
        "    print(\"Note: Macro F1-score is below 0.6. Consider further tuning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqsruwxbHpnG",
        "outputId": "89de1fb6-dec2-4a6b-bd04-e379e685522c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CAM       0.46      0.50      0.48       155\n",
            "          CB       0.58      0.73      0.65       304\n",
            "         CDM       0.44      0.55      0.49       159\n",
            "          CF       0.00      0.00      0.00        12\n",
            "          CM       0.48      0.76      0.59       235\n",
            "         LAM       0.00      0.00      0.00         3\n",
            "          LB       0.73      0.81      0.77       222\n",
            "         LCB       0.36      0.19      0.25       108\n",
            "         LCM       0.08      0.02      0.03        65\n",
            "         LDM       0.20      0.03      0.04        40\n",
            "          LF       0.00      0.00      0.00         3\n",
            "          LM       0.31      0.33      0.32       183\n",
            "          LS       0.00      0.00      0.00        34\n",
            "          LW       0.15      0.03      0.05        62\n",
            "         LWB       0.00      0.00      0.00        14\n",
            "         RAM       0.00      0.00      0.00         4\n",
            "          RB       0.67      0.86      0.75       220\n",
            "         RCB       0.42      0.24      0.31       112\n",
            "         RCM       0.17      0.06      0.09        65\n",
            "         RDM       0.33      0.05      0.09        41\n",
            "          RF       0.00      0.00      0.00         2\n",
            "          RM       0.32      0.35      0.33       185\n",
            "          RS       0.00      0.00      0.00        33\n",
            "          RW       0.00      0.00      0.00        59\n",
            "         RWB       0.00      0.00      0.00        16\n",
            "          ST       0.73      0.90      0.80       349\n",
            "\n",
            "    accuracy                           0.53      2685\n",
            "   macro avg       0.25      0.25      0.23      2685\n",
            "weighted avg       0.46      0.53      0.48      2685\n",
            "\n",
            "Final macro F1-score: 0.2323\n",
            "Note: Macro F1-score is below 0.6. Consider further tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Summary and Reflections\n",
        "\n",
        "In this section, we summarize the insights gained from the different models used in this lab. Each model provided valuable lessons about handling multiclass classification, tuning parameters, and balancing trade-offs.\n",
        "\n",
        "The best performing model in our experiments was the tuned XGBoost classifier. Its ability to handle complex relationships and non-linearities in the data contributed to its superior performance compared to simpler models like Logistic Regression and K-Nearest Neighbors.\n",
        "\n",
        "However, this performance comes with trade-offs. While XGBoost offers high accuracy, it requires longer training times and careful parameter tuning to avoid overfitting. On the other hand, models like Logistic Regression are faster and more interpretable but may underperform with complex datasets. K-Nearest Neighbors offers simplicity but suffers from high prediction time and sensitivity to the choice of neighbors.\n",
        "\n",
        "Throughout this lab, challenges included managing computational time for model training and hyperparameter tuning, especially with grid search on large datasets. Overfitting was also a concern, requiring techniques like cross-validation and careful feature preprocessing.\n",
        "\n",
        "Overall, this lab reinforced the importance of selecting models that balance interpretability, training efficiency, and predictive power, while also emphasizing the role of data preprocessing and hyperparameter optimization.\n"
      ],
      "metadata": {
        "id": "JVXd9NbYIu4R"
      }
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tWhs0v9yXrD"
      },
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://static.tildacdn.one/tild3639-3035-4131-a461-363737393037/noroot.png\" alt=\"University Logo\" width=\"400\" height=\"224\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=5 align=center>\n",
        "Sharif University of Technology\n",
        "<br>\n",
        "Electrical Engineering Department\n",
        "</font>\n",
        "<br>\n",
        "<font size=6>\n",
        "Assignment 10: Deep Neural Networks\n",
        "</font>\n",
        "<br>\n",
        "<font size=4>\n",
        "Zahra Helalizadeh 400102193\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Spring 2025\n",
        "<br>\n",
        "</font>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "</font>\n",
        "</div></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_-fAfhSywOh"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "## 1.1 Problem Statement\n",
        "\n",
        "In this assignment, we aim to build and optimize a deep neural network (DNN) to classify images from the Fashion MNIST dataset. This dataset consists of grayscale images of various clothing items, each labeled with a category such as T-shirt, trousers, dress, etc.\n",
        "\n",
        "The goal is to design a neural network that can accurately classify unseen images into their respective categories. To achieve this, we will train and validate different DNN architectures using Keras, apply 4-fold cross-validation to tune hyperparameters, and evaluate the final model on a separate test set.\n",
        "\n",
        "This task focuses on understanding and improving the performance of deep learning models by experimenting with various components such as optimization algorithms, learning rates, batch sizes, activation functions, regularization techniques, and network depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iIJaoHFyyRW"
      },
      "source": [
        "## 1.2 Dataset Description\n",
        "\n",
        "The Fashion MNIST dataset is a collection of 70,000 grayscale images of clothing items divided into a training set and a test set.\n",
        "\n",
        "Each image is of size 28x28 pixels and represents a single item of clothing, such as a shirt or sneaker. Each image is associated with a label from 10 possible classes.\n",
        "\n",
        "The dataset contains 10 classes, each representing a different category of clothing. The class labels are as follows:\n",
        "\n",
        "1. T-shirt/top\n",
        "2. Trouser\n",
        "3. Pullover\n",
        "4. Dress\n",
        "5. Coat\n",
        "6. Sandal\n",
        "7. Shirt\n",
        "8. Sneaker\n",
        "9. Bag\n",
        "10. Ankle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhf1ki4Dy_S-"
      },
      "source": [
        "## 1.3 Evaluation Metric\n",
        "\n",
        "In this assignment, we will use 4-fold cross-validation to evaluate and tune the deep neural network model. Cross-validation allows us to get a more reliable estimate of the model's performance by training and validating on different subsets of the data.\n",
        "\n",
        "The main metric used for tuning is the average validation accuracy across the 4 folds. This helps us identify the best set of hyperparameters for the model.\n",
        "\n",
        "After selecting the best configuration, we will train the model on the full training data (80% of the dataset) and report the final test accuracy using the remaining 20% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHSz6JcWzls5"
      },
      "source": [
        "# 2. Data Preparation\n",
        "\n",
        "## 2.1 Loading the Dataset\n",
        "\n",
        "We use the Fashion MNIST dataset available in `tensorflow.keras.datasets`. This dataset contains 70,000 grayscale images of fashion items categorized into 10 classes. We will load the data and check the shapes of the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQM-E8EgzneV",
        "outputId": "3cea5b54-deda-4fdd-9dbb-a5d530a72965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Training labels shape: (60000,)\n",
            "Test data shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)\n",
        "print(\"Test data shape:\", x_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXXO5BOpzqUG"
      },
      "source": [
        "## 2.2 Data Normalization\n",
        "\n",
        "The pixel values in the Fashion MNIST dataset range from 0 to 255. To improve the training process and help the neural network converge faster, we normalize the pixel values to the range [0, 1] by dividing each pixel by 255.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV5SCV1Tztf2",
        "outputId": "f4c1f3bf-e6be-4807-c395-beae5996e06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min pixel value after normalization: 0.0\n",
            "Max pixel value after normalization: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Check the range after normalization\n",
        "print(\"Min pixel value after normalization:\", x_train.min())\n",
        "print(\"Max pixel value after normalization:\", x_train.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4cI0iUrzzhk"
      },
      "source": [
        "## 2.3 Train-Test Split (80-20)\n",
        "\n",
        "To evaluate the final performance of the model, we set aside 20% of the entire dataset as a test set. The remaining 80% will be used for training and 4-fold cross-validation during hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdU3Stckzx66",
        "outputId": "44e95ffe-4d5b-400c-9c9f-c3752b935b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training set shape: (56000, 28, 28)\n",
            "Final test set shape: (14000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the original training and test sets\n",
        "x_full = np.concatenate([x_train, x_test], axis=0)\n",
        "y_full = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "# Split into 80% training and 20% test set\n",
        "x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(\n",
        "    x_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
        ")\n",
        "\n",
        "print(\"Final training set shape:\", x_train_final.shape)\n",
        "print(\"Final test set shape:\", x_test_final.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZvVqbrqz5Nz"
      },
      "source": [
        "## 2.4 Label Encoding\n",
        "\n",
        "Since this is a multi-class classification problem, we need to convert the class labels into one-hot encoded vectors. This helps the neural network output probabilities for each class using the softmax activation function in the output layer.\n",
        "\n",
        "We use `to_categorical()` from `tensorflow.keras.utils` to perform one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V68mDTrz7U0",
        "outputId": "b1a9e42b-3e70-4ffd-8fba-4b7d205ca5b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of one-hot encoded training labels: (56000, 10)\n",
            "Shape of one-hot encoded test labels: (14000, 10)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# One-hot encode the training and test labels\n",
        "y_train_final_encoded = to_categorical(y_train_final, num_classes=10)\n",
        "y_test_final_encoded = to_categorical(y_test_final, num_classes=10)\n",
        "\n",
        "print(\"Shape of one-hot encoded training labels:\", y_train_final_encoded.shape)\n",
        "print(\"Shape of one-hot encoded test labels:\", y_test_final_encoded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeziKkeC1azk"
      },
      "source": [
        "# 3. Baseline Neural Network\n",
        "\n",
        "## 3.1 Baseline Model Architecture\n",
        "\n",
        "We begin by building a simple baseline neural network using Keras. This model includes an input layer that flattens the 28x28 image into a 784-dimensional vector, followed by two dense hidden layers with ReLU activation and an output layer with softmax activation for classification.\n",
        "\n",
        "This baseline model will help us establish a reference performance before tuning any hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "0T-7qovY1cih",
        "outputId": "5a20e45f-b47d-4f4b-bef3-810fe433c6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Build the baseline model\n",
        "def create_baseline_model():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Create and summarize the model\n",
        "baseline_model = create_baseline_model()\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAaEfheq1ftj"
      },
      "source": [
        "## 3.2 Cross-Validation Setup\n",
        "\n",
        "To evaluate the performance of the baseline model and future tuned models, we use 4-fold cross-validation. This means the training set is split into 4 parts, and the model is trained 4 times, each time using a different fold as the validation set and the remaining 3 folds as the training set.\n",
        "\n",
        "We use `KFold` from `sklearn.model_selection` to implement this. The average validation accuracy across all folds will be used to assess model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mp0YFyLE1hox"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "# Set up 4-fold cross-validation\n",
        "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Function to perform cross-validation and return average validation accuracy\n",
        "def evaluate_model_cv(model_fn, x_data, y_data, epochs=10, batch_size=32):\n",
        "    val_accuracies = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_data):\n",
        "        x_train_cv, x_val_cv = x_data[train_idx], x_data[val_idx]\n",
        "        y_train_cv, y_val_cv = y_data[train_idx], y_data[val_idx]\n",
        "\n",
        "        model = model_fn()\n",
        "        model.compile(optimizer=Adam(),\n",
        "                      loss=CategoricalCrossentropy(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(x_train_cv, y_train_cv,\n",
        "                            epochs=epochs,\n",
        "                            batch_size=batch_size,\n",
        "                            verbose=0,\n",
        "                            validation_data=(x_val_cv, y_val_cv))\n",
        "\n",
        "        val_acc = history.history['val_accuracy'][-1]\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "    avg_val_accuracy = np.mean(val_accuracies)\n",
        "    print(f\"Average validation accuracy across 4 folds: {avg_val_accuracy:.4f}\")\n",
        "    return avg_val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_XZyKD_1kph"
      },
      "source": [
        "## 3.3 Baseline Training & Evaluation\n",
        "\n",
        "Now, we train and evaluate the baseline model using the 4-fold cross-validation setup. This will give us a baseline average validation accuracy to compare against after tuning the model.\n",
        "\n",
        "We will train for a moderate number of epochs and log the average validation accuracy achieved by this simple architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glDapoTV1mvF",
        "outputId": "8815c6ed-6866-4acb-b5a3-11f0480531f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy across 4 folds: 0.8845\n",
            "Baseline model average validation accuracy: 0.8845\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the baseline model using cross-validation\n",
        "baseline_accuracy = evaluate_model_cv(create_baseline_model, x_train_final, y_train_final_encoded, epochs=10, batch_size=32)\n",
        "\n",
        "print(f\"Baseline model average validation accuracy: {baseline_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzGDJOaO5D_T"
      },
      "source": [
        "# 4. Hyperparameter Tuning Experiments\n",
        "\n",
        "## 4.1 Optimizer Tuning\n",
        "\n",
        "In this experiment, we tune the optimizer used during training. The optimizer controls how the neural network updates its weights based on the loss gradient.\n",
        "\n",
        "We will try five different optimizers:\n",
        "\n",
        "- Stochastic Gradient Descent (SGD)\n",
        "- Adam\n",
        "- RMSprop\n",
        "- Adagrad\n",
        "- Nadam\n",
        "\n",
        "For each optimizer, we will train the same baseline model architecture using 4-fold cross-validation and compare the average validation accuracies to see which optimizer performs best on the Fashion MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJWwP79c5GQR",
        "outputId": "178409f6-d7a3-4036-8ce5-bbf4deabcc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with optimizer: SGD\n",
            "Average validation accuracy across 4 folds: 0.8838\n",
            "SGD average validation accuracy: 0.8838\n",
            "\n",
            "Training with optimizer: Adam\n",
            "Average validation accuracy across 4 folds: 0.8788\n",
            "Adam average validation accuracy: 0.8788\n",
            "\n",
            "Training with optimizer: RMSprop\n",
            "Average validation accuracy across 4 folds: 0.8818\n",
            "RMSprop average validation accuracy: 0.8818\n",
            "\n",
            "Training with optimizer: Adagrad\n",
            "Average validation accuracy across 4 folds: 0.8828\n",
            "Adagrad average validation accuracy: 0.8828\n",
            "\n",
            "Training with optimizer: Nadam\n",
            "Average validation accuracy across 4 folds: 0.8839\n",
            "Nadam average validation accuracy: 0.8839\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Nadam\n",
        "\n",
        "optimizers = {\n",
        "    'SGD': SGD(),\n",
        "    'Adam': Adam(),\n",
        "    'RMSprop': RMSprop(),\n",
        "    'Adagrad': Adagrad(),\n",
        "    'Nadam': Nadam()\n",
        "}\n",
        "\n",
        "def create_model_with_optimizer(optimizer):\n",
        "    def model_fn():\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(28, 28)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "    return model_fn\n",
        "\n",
        "results = {}\n",
        "\n",
        "for opt_name, opt in optimizers.items():\n",
        "    print(f\"Training with optimizer: {opt_name}\")\n",
        "    model_fn = create_model_with_optimizer(opt)\n",
        "    avg_val_acc = evaluate_model_cv(model_fn, x_train_final, y_train_final_encoded, epochs=10, batch_size=32)\n",
        "    results[opt_name] = avg_val_acc\n",
        "    print(f\"{opt_name} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6yozhHt5In6"
      },
      "source": [
        "### Results and Discussion\n",
        "\n",
        "Among the tested optimizers, Nadam achieved the highest average validation accuracy of **0.8839**, followed closely by SGD (**0.8838**) and Adagrad (**0.8828**). RMSprop (**0.8818**) and Adam (**0.8788**) slightly lagged behind.\n",
        "\n",
        "Interestingly, Nadam and SGD—though conceptually different (one adaptive, the other momentum-based)—performed nearly identically, suggesting that both can be strong choices for Fashion MNIST under this architecture.\n",
        "\n",
        "This outcome highlights that while adaptive optimizers like Adam and RMSprop are often preferred for faster convergence, traditional optimizers like SGD can still be highly competitive when used with appropriate settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx6HH52hAIfY"
      },
      "source": [
        "## 4.2 Learning Rate Tuning\n",
        "\n",
        "In this experiment, we tune the learning rate, which controls the step size at each iteration while moving toward a minimum of the loss function.\n",
        "\n",
        "We keep the optimizer fixed as Adam and try different learning rates to observe their effect on model performance. The learning rates we test are:\n",
        "\n",
        "- 0.01 (1e-2)\n",
        "- 0.001 (1e-3)\n",
        "- 0.0001 (1e-4)\n",
        "- 0.00001 (1e-5)\n",
        "- 0.000001 (1e-6)\n",
        "\n",
        "This will help us understand how sensitive the model is to the choice of learning rate and identify an optimal value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9tlrr2rALFc",
        "outputId": "8c61fddd-0c37-4ee5-b41d-9d5bb70da3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.01\n",
            "Average validation accuracy across 4 folds: 0.8820\n",
            "Learning rate 0.01 average validation accuracy: 0.8820\n",
            "\n",
            "Training with learning rate: 0.001\n",
            "Average validation accuracy across 4 folds: 0.8840\n",
            "Learning rate 0.001 average validation accuracy: 0.8840\n",
            "\n",
            "Training with learning rate: 0.0001\n",
            "Average validation accuracy across 4 folds: 0.8827\n",
            "Learning rate 0.0001 average validation accuracy: 0.8827\n",
            "\n",
            "Training with learning rate: 1e-05\n",
            "Average validation accuracy across 4 folds: 0.8850\n",
            "Learning rate 1e-05 average validation accuracy: 0.8850\n",
            "\n",
            "Training with learning rate: 1e-06\n",
            "Average validation accuracy across 4 folds: 0.8858\n",
            "Learning rate 1e-06 average validation accuracy: 0.8858\n",
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "\n",
        "def create_model_with_lr(lr):\n",
        "    def model_fn():\n",
        "        optimizer = Adam(learning_rate=lr)\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(28, 28)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "    return model_fn\n",
        "\n",
        "lr_results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "    model_fn = create_model_with_lr(lr)\n",
        "    avg_val_acc = evaluate_model_cv(model_fn, x_train_final, y_train_final_encoded, epochs=10, batch_size=32)\n",
        "    lr_results[lr] = avg_val_acc\n",
        "    print(f\"Learning rate {lr} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIbUpg-gAOQR"
      },
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The experiment demonstrates that extremely low learning rates provided the best validation accuracy. Specifically, a learning rate of **1e-06** resulted in the highest average validation accuracy of **0.8858**, followed by **1e-05** (**0.8850**) and **0.001** (**0.8840**).\n",
        "\n",
        "Higher learning rates such as **0.01** led to slightly reduced performance (**0.8820**), likely due to instability or overshooting during training.\n",
        "\n",
        "These results indicate that for this model and dataset, smaller learning rates allow finer convergence and better generalization—although they may increase training time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Learning Rate Decay\n",
        "\n",
        "In this section, we explore learning rate decay strategies. Instead of using a fixed learning rate, we reduce the learning rate during training to allow finer convergence near minima.\n",
        "\n",
        "We will experiment with the following learning rate decay methods using the Adam optimizer:\n",
        "\n",
        "1. No decay (constant learning rate)\n",
        "2. ExponentialDecay\n",
        "3. PiecewiseConstantDecay\n",
        "4. PolynomialDecay\n",
        "5. InverseTimeDecay\n",
        "\n",
        "Each strategy will be evaluated using 4-fold cross-validation, and the average validation accuracy will be logged."
      ],
      "metadata": {
        "id": "HYeYjV25O58O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, InverseTimeDecay\n",
        "\n",
        "def create_model_with_schedule(lr_schedule):\n",
        "    def model_fn():\n",
        "        optimizer = Adam(learning_rate=lr_schedule)\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(28, 28)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "    return model_fn\n",
        "\n",
        "# Different learning rate schedules\n",
        "schedules = {\n",
        "    \"No Decay\": 0.001,\n",
        "    \"ExponentialDecay\": ExponentialDecay(initial_learning_rate=0.01, decay_steps=1000, decay_rate=0.9),\n",
        "    \"PiecewiseConstantDecay\": PiecewiseConstantDecay(boundaries=[1000, 2000], values=[0.01, 0.001, 0.0001]),\n",
        "    \"PolynomialDecay\": PolynomialDecay(initial_learning_rate=0.01, decay_steps=2000, end_learning_rate=0.0001, power=2.0),\n",
        "    \"InverseTimeDecay\": InverseTimeDecay(initial_learning_rate=0.01, decay_steps=1000, decay_rate=0.5)\n",
        "}\n",
        "\n",
        "lr_decay_results = {}\n",
        "\n",
        "for name, schedule in schedules.items():\n",
        "    print(f\"Training with schedule: {name}\")\n",
        "    model_fn = create_model_with_schedule(schedule)\n",
        "    avg_val_acc = evaluate_model_cv(model_fn, x_train_final, y_train_final_encoded, epochs=10, batch_size=32)\n",
        "    lr_decay_results[name] = avg_val_acc\n",
        "    print(f\"{name} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DenZB1OO6WL",
        "outputId": "393c9921-1a64-4e27-dd1f-c3611e80ba8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with schedule: No Decay\n",
            "Average validation accuracy across 4 folds: 0.8844\n",
            "No Decay average validation accuracy: 0.8844\n",
            "\n",
            "Training with schedule: ExponentialDecay\n",
            "Average validation accuracy across 4 folds: 0.8856\n",
            "ExponentialDecay average validation accuracy: 0.8856\n",
            "\n",
            "Training with schedule: PiecewiseConstantDecay\n",
            "Average validation accuracy across 4 folds: 0.8816\n",
            "PiecewiseConstantDecay average validation accuracy: 0.8816\n",
            "\n",
            "Training with schedule: PolynomialDecay\n",
            "Average validation accuracy across 4 folds: 0.8846\n",
            "PolynomialDecay average validation accuracy: 0.8846\n",
            "\n",
            "Training with schedule: InverseTimeDecay\n",
            "Average validation accuracy across 4 folds: 0.8848\n",
            "InverseTimeDecay average validation accuracy: 0.8848\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "Among the various learning rate decay strategies, **ExponentialDecay** achieved the highest validation accuracy at **0.8856**, narrowly outperforming **InverseTimeDecay** (**0.8848**) and **PolynomialDecay** (**0.8846**).\n",
        "\n",
        "The use of **No Decay** yielded **0.8844**, indicating that decay schedules, though subtle in impact, do contribute positively to model generalization.\n",
        "\n",
        "On the other hand, **PiecewiseConstantDecay** underperformed with **0.8816**, suggesting that abrupt shifts in learning rate may not be optimal for this task."
      ],
      "metadata": {
        "id": "H9nWtyH4PEKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Batch Size Tuning\n",
        "\n",
        "In this section, we experiment with different batch sizes to investigate how it affects training dynamics and validation accuracy. The batch size determines how many training examples are processed before the model updates its weights.\n",
        "\n",
        "Smaller batch sizes offer more weight updates per epoch but can be noisier. Larger batch sizes provide more stable gradients but may converge slower or get stuck in sharp minima.\n",
        "\n",
        "We will evaluate the model with batch sizes of 16, 32, 64, 128, and 256."
      ],
      "metadata": {
        "id": "xZRlFvToW5xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [16, 32, 64, 128, 256]\n",
        "batch_size_results = {}\n",
        "\n",
        "def create_model_for_batch():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    print(f\"Training with batch size: {bs}\")\n",
        "    avg_val_acc = evaluate_model_cv(create_model_for_batch, x_train_final, y_train_final_encoded, epochs=10, batch_size=bs)\n",
        "    batch_size_results[bs] = avg_val_acc\n",
        "    print(f\"Batch size {bs} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3lxfh5BW8ZU",
        "outputId": "742194f7-0e91-4167-8407-c3821cb9010e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with batch size: 16\n",
            "Average validation accuracy across 4 folds: 0.8855\n",
            "Batch size 16 average validation accuracy: 0.8855\n",
            "\n",
            "Training with batch size: 32\n",
            "Average validation accuracy across 4 folds: 0.8804\n",
            "Batch size 32 average validation accuracy: 0.8804\n",
            "\n",
            "Training with batch size: 64\n",
            "Average validation accuracy across 4 folds: 0.8828\n",
            "Batch size 64 average validation accuracy: 0.8828\n",
            "\n",
            "Training with batch size: 128\n",
            "Average validation accuracy across 4 folds: 0.8865\n",
            "Batch size 128 average validation accuracy: 0.8865\n",
            "\n",
            "Training with batch size: 256\n",
            "Average validation accuracy across 4 folds: 0.8787\n",
            "Batch size 256 average validation accuracy: 0.8787\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The highest validation accuracy was observed with a batch size of **128**, achieving **0.8865**. Smaller batches like **16** also performed well (**0.8855**), while **256** yielded the lowest accuracy (**0.8787**).\n",
        "\n",
        "This suggests that moderate batch sizes (64–128) offer the best balance of stability and generalization. Very small batches introduce noise, while very large batches may hinder convergence or cause overfitting.\n"
      ],
      "metadata": {
        "id": "kcYhOVP6W_To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Activation Function Tuning\n",
        "\n",
        "In this section, we test different activation functions to study their effect on the learning process and model performance.\n",
        "\n",
        "The activation function introduces non-linearity, which is essential for the network to learn complex patterns. We try the following activation functions:\n",
        "- ReLU\n",
        "- Sigmoid\n",
        "- Tanh\n",
        "- Leaky ReLU\n",
        "- SELU\n",
        "\n",
        "We apply the activation function to all hidden layers and keep the rest of the model (e.g., optimizer, loss function, architecture) the same."
      ],
      "metadata": {
        "id": "ylETlCs8dPOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "activations = {\n",
        "    'relu': lambda: Dense(128, activation='relu'),\n",
        "    'sigmoid': lambda: Dense(128, activation='sigmoid'),\n",
        "    'tanh': lambda: Dense(128, activation='tanh'),\n",
        "    'leaky_relu': lambda: (Dense(128), LeakyReLU(alpha=0.1)),\n",
        "    'selu': lambda: Dense(128, activation='selu')\n",
        "}\n",
        "\n",
        "activation_results = {}\n",
        "\n",
        "def create_model_with_activation(activation_key):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    if activation_key == 'leaky_relu':\n",
        "        layer, activation = activations[activation_key]()\n",
        "        model.add(layer)\n",
        "        model.add(activation)\n",
        "        model.add(Dense(64))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "    else:\n",
        "        model.add(activations[activation_key]())\n",
        "        model.add(Dense(64, activation=activation_key))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "for act in activations:\n",
        "    print(f\"Training with activation: {act}\")\n",
        "    avg_val_acc = evaluate_model_cv(lambda: create_model_with_activation(act), x_train_final, y_train_final_encoded, epochs=10, batch_size=64)\n",
        "    activation_results[act] = avg_val_acc\n",
        "    print(f\"{act} activation average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dgi7A0OdO8F",
        "outputId": "cc7efb16-8eae-4581-d1a1-aad6081599e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with activation: relu\n",
            "Average validation accuracy across 4 folds: 0.8855\n",
            "relu activation average validation accuracy: 0.8855\n",
            "\n",
            "Training with activation: sigmoid\n",
            "Average validation accuracy across 4 folds: 0.8802\n",
            "sigmoid activation average validation accuracy: 0.8802\n",
            "\n",
            "Training with activation: tanh\n",
            "Average validation accuracy across 4 folds: 0.8861\n",
            "tanh activation average validation accuracy: 0.8861\n",
            "\n",
            "Training with activation: leaky_relu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy across 4 folds: 0.8802\n",
            "leaky_relu activation average validation accuracy: 0.8802\n",
            "\n",
            "Training with activation: selu\n",
            "Average validation accuracy across 4 folds: 0.8802\n",
            "selu activation average validation accuracy: 0.8802\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The **tanh** activation function slightly outperformed the others with an average accuracy of **0.8861**, followed closely by **relu** at **0.8855**. Other activations like **sigmoid**, **leaky_relu**, and **selu** all plateaued around **0.8802**.\n",
        "\n",
        "This indicates that **tanh** and **relu** are most effective for this task, likely due to better gradient flow and non-linearity. Despite their theoretical advantages, leaky_relu and selu didn't provide additional benefits here."
      ],
      "metadata": {
        "id": "KaMGa0BydVAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Weight Initialization Tuning\n",
        "\n",
        "In this section, we experiment with different weight initialization strategies. Proper initialization can significantly impact how quickly and effectively the model learns.\n",
        "\n",
        "We test the following initializers:\n",
        "- `he_uniform`\n",
        "- `he_normal`\n",
        "- `glorot_uniform`\n",
        "- `glorot_normal`\n",
        "- `random_normal`\n",
        "\n",
        "These are applied to the weights of the Dense layers in the model."
      ],
      "metadata": {
        "id": "0Uee3oDyhRsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import HeUniform, HeNormal, GlorotUniform, GlorotNormal, RandomNormal\n",
        "\n",
        "initializers = {\n",
        "    'he_uniform': HeUniform(),\n",
        "    'he_normal': HeNormal(),\n",
        "    'glorot_uniform': GlorotUniform(),\n",
        "    'glorot_normal': GlorotNormal(),\n",
        "    'random_normal': RandomNormal(mean=0.0, stddev=0.05)\n",
        "}\n",
        "\n",
        "initializer_results = {}\n",
        "\n",
        "def create_model_with_initializer(initializer):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer=initializer))\n",
        "    model.add(Dense(64, activation='relu', kernel_initializer=initializer))\n",
        "    model.add(Dense(10, activation='softmax', kernel_initializer=initializer))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "for name, init in initializers.items():\n",
        "    print(f\"Training with initializer: {name}\")\n",
        "    avg_val_acc = evaluate_model_cv(lambda: create_model_with_initializer(init), x_train_final, y_train_final_encoded, epochs=10, batch_size=64)\n",
        "    initializer_results[name] = avg_val_acc\n",
        "    print(f\"{name} initializer average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uifXyXbzhUGd",
        "outputId": "1b8491ee-7ab5-46b4-ea9d-bf2f3179ebdf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with initializer: he_uniform\n",
            "Average validation accuracy across 4 folds: 0.8846\n",
            "he_uniform initializer average validation accuracy: 0.8846\n",
            "\n",
            "Training with initializer: he_normal\n",
            "Average validation accuracy across 4 folds: 0.8849\n",
            "he_normal initializer average validation accuracy: 0.8849\n",
            "\n",
            "Training with initializer: glorot_uniform\n",
            "Average validation accuracy across 4 folds: 0.8827\n",
            "glorot_uniform initializer average validation accuracy: 0.8827\n",
            "\n",
            "Training with initializer: glorot_normal\n",
            "Average validation accuracy across 4 folds: 0.8837\n",
            "glorot_normal initializer average validation accuracy: 0.8837\n",
            "\n",
            "Training with initializer: random_normal\n",
            "Average validation accuracy across 4 folds: 0.8830\n",
            "random_normal initializer average validation accuracy: 0.8830\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The best-performing initializers were **he_normal** (**0.8849**) and **he_uniform** (**0.8846**), which are well-suited for ReLU activations. **glorot_normal** and **glorot_uniform** also performed reasonably well (around **0.883–0.8837**).\n",
        "\n",
        "The **random_normal** initializer achieved the lowest performance at **0.8830**, confirming that specialized initializers tailored to activation types (e.g., He for ReLU) provide more stable convergence and better generalization."
      ],
      "metadata": {
        "id": "6yPgoLMDhWXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 Network Architecture Tuning\n",
        "\n",
        "In this section, we experiment with varying the depth and width of the neural network. The number of hidden layers and the number of neurons in each layer directly affect the model's capacity and ability to generalize.\n",
        "\n",
        "We explore different architectures by adjusting:\n",
        "- Number of hidden layers: 2, 3, 4, 5\n",
        "- Neurons per layer: 64, 128, 256\n",
        "\n",
        "The activation function is kept as ReLU, and the optimizer is Adam for consistency."
      ],
      "metadata": {
        "id": "JO3gq3wtn1bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_architecture_model(num_layers, units_per_layer):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    for _ in range(num_layers):\n",
        "        model.add(Dense(units_per_layer, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "H8DK23kWn33H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architectures = [\n",
        "    (2, 64),\n",
        "    (2, 128),\n",
        "    (3, 128),\n",
        "    (4, 256),\n",
        "    (5, 256)\n",
        "]\n",
        "\n",
        "architecture_results = {}\n",
        "\n",
        "for layers, units in architectures:\n",
        "    name = f\"{layers} layers, {units} units\"\n",
        "    print(f\"Training with architecture: {name}\")\n",
        "    avg_val_acc = evaluate_model_cv(lambda: create_architecture_model(layers, units), x_train_final, y_train_final_encoded, epochs=10, batch_size=64)\n",
        "    architecture_results[name] = avg_val_acc\n",
        "    print(f\"{name} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doTsmDJBn578",
        "outputId": "65dde46f-ea7d-444a-cb8b-06db58613d05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with architecture: 2 layers, 64 units\n",
            "Average validation accuracy across 4 folds: 0.8787\n",
            "2 layers, 64 units average validation accuracy: 0.8787\n",
            "\n",
            "Training with architecture: 2 layers, 128 units\n",
            "Average validation accuracy across 4 folds: 0.8826\n",
            "2 layers, 128 units average validation accuracy: 0.8826\n",
            "\n",
            "Training with architecture: 3 layers, 128 units\n",
            "Average validation accuracy across 4 folds: 0.8869\n",
            "3 layers, 128 units average validation accuracy: 0.8869\n",
            "\n",
            "Training with architecture: 4 layers, 256 units\n",
            "Average validation accuracy across 4 folds: 0.8853\n",
            "4 layers, 256 units average validation accuracy: 0.8853\n",
            "\n",
            "Training with architecture: 5 layers, 256 units\n",
            "Average validation accuracy across 4 folds: 0.8856\n",
            "5 layers, 256 units average validation accuracy: 0.8856\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The architecture with **3 layers and 128 units** achieved the best validation accuracy at **0.8869**. The **4-layer 256-unit** model was close behind (**0.8853**), while deeper architectures (5 layers, 256 units) saw no further improvement (**0.8856**).\n",
        "\n",
        "Shallower networks like **2 layers, 64 units** underperformed (**0.8787**), indicating insufficient model capacity.\n",
        "\n",
        "These results confirm that moderate depth and width offer the best trade-off between underfitting and overfitting for this task."
      ],
      "metadata": {
        "id": "_yNdUJ4An76v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 L1 and L2 Regularization on Weights\n",
        "\n",
        "In this section, we apply different types and strengths of regularization to the weights of the dense layers using the `kernel_regularizer` argument in Keras. Regularization helps prevent overfitting by penalizing large weights.\n",
        "\n",
        "We test the following variations:\n",
        "- L1 regularization with different strengths\n",
        "- L2 regularization with different strengths\n",
        "- L1_L2 combined regularization\n",
        "\n",
        "The model architecture is fixed to 2 hidden layers with 128 neurons each and ReLU activation. Optimizer is Adam."
      ],
      "metadata": {
        "id": "4m2ZNn1Tu8yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def create_regularized_model(regularizer):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=regularizer))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=regularizer))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "J8uhRj78u_Nn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regularization_configs = {\n",
        "    \"L1 (0.001)\": regularizers.l1(0.001),\n",
        "    \"L2 (0.001)\": regularizers.l2(0.001),\n",
        "    \"L2 (0.0001)\": regularizers.l2(0.0001),\n",
        "    \"L1_L2 (0.001, 0.001)\": regularizers.l1_l2(l1=0.001, l2=0.001),\n",
        "    \"L1_L2 (0.0005, 0.0005)\": regularizers.l1_l2(l1=0.0005, l2=0.0005)\n",
        "}\n",
        "\n",
        "regularization_results = {}\n",
        "\n",
        "for name, reg in regularization_configs.items():\n",
        "    print(f\"Training with regularization: {name}\")\n",
        "    avg_val_acc = evaluate_model_cv(lambda: create_regularized_model(reg), x_train_final, y_train_final_encoded, epochs=10, batch_size=64)\n",
        "    regularization_results[name] = avg_val_acc\n",
        "    print(f\"{name} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaEVGsY-vBSh",
        "outputId": "b162cb09-8411-4558-bfc6-51163b56e5de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with regularization: L1 (0.001)\n",
            "Average validation accuracy across 4 folds: 0.8365\n",
            "L1 (0.001) average validation accuracy: 0.8365\n",
            "\n",
            "Training with regularization: L2 (0.001)\n",
            "Average validation accuracy across 4 folds: 0.8653\n",
            "L2 (0.001) average validation accuracy: 0.8653\n",
            "\n",
            "Training with regularization: L2 (0.0001)\n",
            "Average validation accuracy across 4 folds: 0.8814\n",
            "L2 (0.0001) average validation accuracy: 0.8814\n",
            "\n",
            "Training with regularization: L1_L2 (0.001, 0.001)\n",
            "Average validation accuracy across 4 folds: 0.8389\n",
            "L1_L2 (0.001, 0.001) average validation accuracy: 0.8389\n",
            "\n",
            "Training with regularization: L1_L2 (0.0005, 0.0005)\n",
            "Average validation accuracy across 4 folds: 0.8438\n",
            "L1_L2 (0.0005, 0.0005) average validation accuracy: 0.8438\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The regularization with **L2 (0.0001)** gave the best accuracy (**0.8814**), showing a beneficial impact without excessive constraint.\n",
        "\n",
        "In contrast, **L1 (0.001)** and **L1_L2 (0.001, 0.001)** significantly degraded performance (**0.8365** and **0.8389** respectively), likely due to excessive regularization.\n",
        "\n",
        "Overall, **lightweight L2 regularization** appears to enhance generalization, whereas heavy or L1-based penalties may overly restrict model capacity."
      ],
      "metadata": {
        "id": "p9EowotivEGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9 L1 and L2 Regularization on Activity\n",
        "\n",
        "In this section, we explore regularization applied to the activations of the layers using the `activity_regularizer` argument in Keras. This helps constrain the output of each layer, potentially improving generalization by preventing large activation values.\n",
        "\n",
        "We experiment with the following configurations:\n",
        "L1 activity regularization with different strengths  \n",
        "L2 activity regularization with different strengths  \n",
        "Combined L1_L2 activity regularization\n",
        "\n",
        "We use a simple architecture with two dense layers of 128 neurons and ReLU activation. The optimizer is Adam."
      ],
      "metadata": {
        "id": "VwIOH6UVz6aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_activity_regularized_model(activity_reg):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation='relu', activity_regularizer=activity_reg))\n",
        "    model.add(Dense(128, activation='relu', activity_regularizer=activity_reg))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "XOhTFRjpz8PX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity_regularization_configs = {\n",
        "    \"L1 (0.001)\": regularizers.l1(0.001),\n",
        "    \"L2 (0.001)\": regularizers.l2(0.001),\n",
        "    \"L2 (0.0001)\": regularizers.l2(0.0001),\n",
        "    \"L1_L2 (0.001, 0.001)\": regularizers.l1_l2(l1=0.001, l2=0.001),\n",
        "    \"L1_L2 (0.0005, 0.0005)\": regularizers.l1_l2(l1=0.0005, l2=0.0005)\n",
        "}\n",
        "\n",
        "activity_regularization_results = {}\n",
        "\n",
        "for name, reg in activity_regularization_configs.items():\n",
        "    print(f\"Training with activity regularization: {name}\")\n",
        "    avg_val_acc = evaluate_model_cv(lambda: create_activity_regularized_model(reg), x_train_final, y_train_final_encoded, epochs=10, batch_size=64)\n",
        "    activity_regularization_results[name] = avg_val_acc\n",
        "    print(f\"{name} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXEICya4z-KU",
        "outputId": "5d2b0e10-2ead-4fdc-ab0c-623b7e628611"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with activity regularization: L1 (0.001)\n",
            "Average validation accuracy across 4 folds: 0.4365\n",
            "L1 (0.001) average validation accuracy: 0.4365\n",
            "\n",
            "Training with activity regularization: L2 (0.001)\n",
            "Average validation accuracy across 4 folds: 0.8206\n",
            "L2 (0.001) average validation accuracy: 0.8206\n",
            "\n",
            "Training with activity regularization: L2 (0.0001)\n",
            "Average validation accuracy across 4 folds: 0.8716\n",
            "L2 (0.0001) average validation accuracy: 0.8716\n",
            "\n",
            "Training with activity regularization: L1_L2 (0.001, 0.001)\n",
            "Average validation accuracy across 4 folds: 0.4354\n",
            "L1_L2 (0.001, 0.001) average validation accuracy: 0.4354\n",
            "\n",
            "Training with activity regularization: L1_L2 (0.0005, 0.0005)\n",
            "Average validation accuracy across 4 folds: 0.6674\n",
            "L1_L2 (0.0005, 0.0005) average validation accuracy: 0.6674\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "The regularization with **L2 (0.0001)** gave the best accuracy (**0.8716**), effectively controlling activation magnitudes without overly constraining the network.\n",
        "\n",
        "In contrast, **L1 (0.001)** and **L1\\_L2 (0.001, 0.001)** significantly hurt performance (**0.4365** and **0.4354** respectively), likely due to excessive sparsity imposed on activations.\n",
        "\n",
        "**L2 (0.001)** also yielded strong performance (**0.8206**), though slightly less than the lighter version.\n",
        "\n",
        "The combined **L1\\_L2 (0.0005, 0.0005)** regularization gave moderate accuracy (**0.6674**), suggesting that even balanced regularization can be suboptimal when applied to activations.\n",
        "\n",
        "Overall, **light L2 activity regularization** achieved the best trade-off between encouraging generalization and maintaining learning capacity.\n"
      ],
      "metadata": {
        "id": "stMGADix0Bpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.10 Dropout Rate Tuning\n",
        "\n",
        "In this section, we evaluate how different dropout rates affect model performance. Dropout is a regularization technique that randomly sets a fraction of input units to zero during training, helping prevent overfitting.\n",
        "\n",
        "We experiment with the following dropout rates: 0.1, 0.2, 0.3, 0.4, and 0.5.\n",
        "\n",
        "All models use the same architecture with two dense layers of 128 neurons, ReLU activation, and dropout inserted after each dense layer. The optimizer used is Adam."
      ],
      "metadata": {
        "id": "sS1iAzS7-ooJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dropout_model(dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "vsTmHNAg-rRe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "dropout_results = {}\n",
        "\n",
        "for rate in dropout_rates:\n",
        "    print(f\"Training with dropout rate: {rate}\")\n",
        "    avg_val_acc = evaluate_model_cv(lambda: create_dropout_model(rate), x_train_final, y_train_final_encoded, epochs=10, batch_size=64)\n",
        "    dropout_results[rate] = avg_val_acc\n",
        "    print(f\"Dropout rate {rate} average validation accuracy: {avg_val_acc:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB2jLhN0-tn7",
        "outputId": "cbfb5fc2-32d4-4a5f-9a98-794d3dd4b2e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with dropout rate: 0.1\n",
            "Average validation accuracy across 4 folds: 0.8847\n",
            "Dropout rate 0.1 average validation accuracy: 0.8847\n",
            "\n",
            "Training with dropout rate: 0.2\n",
            "Average validation accuracy across 4 folds: 0.8818\n",
            "Dropout rate 0.2 average validation accuracy: 0.8818\n",
            "\n",
            "Training with dropout rate: 0.3\n",
            "Average validation accuracy across 4 folds: 0.8816\n",
            "Dropout rate 0.3 average validation accuracy: 0.8816\n",
            "\n",
            "Training with dropout rate: 0.4\n",
            "Average validation accuracy across 4 folds: 0.8740\n",
            "Dropout rate 0.4 average validation accuracy: 0.8740\n",
            "\n",
            "Training with dropout rate: 0.5\n",
            "Average validation accuracy across 4 folds: 0.8688\n",
            "Dropout rate 0.5 average validation accuracy: 0.8688\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and Discussion\n",
        "\n",
        "Dropout helped mitigate overfitting across all tested rates, though the strength of its effect varied:\n",
        "\n",
        "The best performance came from a **dropout rate of 0.1**, yielding the highest average accuracy (**0.8847**).\n",
        "\n",
        "Slightly higher rates (0.2 and 0.3) maintained high performance (**0.8818** and **0.8816**, respectively), though with minor degradation.\n",
        "\n",
        "Further increases in dropout (0.4 and 0.5) led to more noticeable drops in accuracy (**0.8740** and **0.8688**), likely due to underutilization of model capacity.\n",
        "\n",
        "Hence, a **low dropout rate (0.1–0.2)** provided the most effective regularization without compromising performance."
      ],
      "metadata": {
        "id": "fBaM8toy-v4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Final Model & Test Set Evaluation\n",
        "\n",
        "## 5.1 Summary of Best Hyperparameters\n",
        "\n",
        "The best hyperparameter configuration was selected based on the highest average validation accuracy across 4 folds during cross-validation.\n",
        "\n",
        "**Optimizer:** Nadam (0.8839)  \n",
        "**Learning Rate:** 1e-06 (0.8858)  \n",
        "**Learning Rate Schedule:** ExponentialDecay (0.8856)  \n",
        "**Batch Size:** 128 (0.8865)  \n",
        "**Activation Function:** tanh (0.8861)  \n",
        "**Weight Initializer:** he_normal (0.8849)  \n",
        "**Network Architecture:** 3 layers, 128 units (0.8869)  \n",
        "**Weight Regularization:** L2 (0.0001) (0.8814)  \n",
        "**Activity Regularization:** L2 (0.0001) (0.8716)  \n",
        "**Dropout Rate:** 0.1 (0.8847)  \n",
        "\n",
        "These values represent the optimal trade-offs between model complexity, regularization, and generalization performance.\n",
        "\n",
        "## Summary Table of Best Results by Section\n",
        "\n",
        "| Hyperparameter          | Best Value           | Validation Accuracy |\n",
        "|------------------------|----------------------|---------------------|\n",
        "| Optimizer              | Nadam                | 0.8839              |\n",
        "| Learning Rate          | 1e-06                | 0.8858              |\n",
        "| Learning Rate Schedule | ExponentialDecay     | 0.8856              |\n",
        "| Batch Size             | 128                  | 0.8865              |\n",
        "| Activation Function    | tanh                 | 0.8861              |\n",
        "| Weight Initializer     | he_normal            | 0.8849              |\n",
        "| Architecture           | 3 layers, 128 units  | 0.8869              |\n",
        "| Weight Regularization  | L2 (0.0001)          | 0.8814              |\n",
        "| Activity Regularization| L2 (0.0001)          | 0.8716              |\n",
        "| Dropout Rate           | 0.1                  | 0.8847              |\n",
        "\n",
        "\n",
        "These results guided the final model selection used for the test set evaluation.\n"
      ],
      "metadata": {
        "id": "o-o0h45h___b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Training on Full Training Data\n",
        "\n",
        "The final model is trained on the entire training dataset (80% of the total data) using the best hyperparameters identified in the previous section."
      ],
      "metadata": {
        "id": "Vzt2S1Y_ARZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Flatten the input data if not already flattened\n",
        "import numpy as np\n",
        "x_train_final_flat = x_train_final.reshape(x_train_final.shape[0], -1)\n",
        "\n",
        "input_shape = x_train_final_flat.shape[1]\n",
        "\n",
        "def build_final_model():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_shape,)))  # Use Input layer\n",
        "    model.add(Dense(128, kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(128, kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(128, kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(y_train_final_encoded.shape[1], activation='softmax'))\n",
        "\n",
        "    optimizer = Nadam(learning_rate=1e-6)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "final_model = build_final_model()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = final_model.fit(\n",
        "    x_train_final_flat,\n",
        "    y_train_final_encoded,\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE9OUpSsATF_",
        "outputId": "1252c4e3-e2c1-4496-ea83-24f54cd1240c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "350/350 - 7s - 21ms/step - accuracy: 0.0985 - loss: 2.5893 - val_accuracy: 0.1323 - val_loss: 2.4267\n",
            "Epoch 2/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.1383 - loss: 2.4191 - val_accuracy: 0.1991 - val_loss: 2.2671\n",
            "Epoch 3/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.1899 - loss: 2.2856 - val_accuracy: 0.2684 - val_loss: 2.1365\n",
            "Epoch 4/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.2435 - loss: 2.1704 - val_accuracy: 0.3318 - val_loss: 2.0238\n",
            "Epoch 5/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.3040 - loss: 2.0651 - val_accuracy: 0.3837 - val_loss: 1.9236\n",
            "Epoch 6/100\n",
            "350/350 - 5s - 16ms/step - accuracy: 0.3462 - loss: 1.9746 - val_accuracy: 0.4334 - val_loss: 1.8327\n",
            "Epoch 7/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.3894 - loss: 1.8883 - val_accuracy: 0.4767 - val_loss: 1.7496\n",
            "Epoch 8/100\n",
            "350/350 - 5s - 13ms/step - accuracy: 0.4261 - loss: 1.8149 - val_accuracy: 0.5114 - val_loss: 1.6738\n",
            "Epoch 9/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.4538 - loss: 1.7415 - val_accuracy: 0.5421 - val_loss: 1.6045\n",
            "Epoch 10/100\n",
            "350/350 - 4s - 13ms/step - accuracy: 0.4848 - loss: 1.6735 - val_accuracy: 0.5637 - val_loss: 1.5416\n",
            "Epoch 11/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.5084 - loss: 1.6106 - val_accuracy: 0.5819 - val_loss: 1.4842\n",
            "Epoch 12/100\n",
            "350/350 - 6s - 18ms/step - accuracy: 0.5283 - loss: 1.5579 - val_accuracy: 0.5955 - val_loss: 1.4318\n",
            "Epoch 13/100\n",
            "350/350 - 5s - 13ms/step - accuracy: 0.5454 - loss: 1.5027 - val_accuracy: 0.6057 - val_loss: 1.3839\n",
            "Epoch 14/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.5604 - loss: 1.4561 - val_accuracy: 0.6153 - val_loss: 1.3398\n",
            "Epoch 15/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.5745 - loss: 1.4120 - val_accuracy: 0.6227 - val_loss: 1.2993\n",
            "Epoch 16/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.5877 - loss: 1.3702 - val_accuracy: 0.6288 - val_loss: 1.2620\n",
            "Epoch 17/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.5975 - loss: 1.3332 - val_accuracy: 0.6352 - val_loss: 1.2276\n",
            "Epoch 18/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.6085 - loss: 1.2977 - val_accuracy: 0.6397 - val_loss: 1.1959\n",
            "Epoch 19/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.6180 - loss: 1.2640 - val_accuracy: 0.6441 - val_loss: 1.1664\n",
            "Epoch 20/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.6231 - loss: 1.2325 - val_accuracy: 0.6491 - val_loss: 1.1392\n",
            "Epoch 21/100\n",
            "350/350 - 7s - 19ms/step - accuracy: 0.6318 - loss: 1.2022 - val_accuracy: 0.6540 - val_loss: 1.1141\n",
            "Epoch 22/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.6398 - loss: 1.1814 - val_accuracy: 0.6604 - val_loss: 1.0909\n",
            "Epoch 23/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.6465 - loss: 1.1538 - val_accuracy: 0.6658 - val_loss: 1.0692\n",
            "Epoch 24/100\n",
            "350/350 - 6s - 18ms/step - accuracy: 0.6501 - loss: 1.1326 - val_accuracy: 0.6719 - val_loss: 1.0491\n",
            "Epoch 25/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.6581 - loss: 1.1108 - val_accuracy: 0.6767 - val_loss: 1.0302\n",
            "Epoch 26/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.6623 - loss: 1.0912 - val_accuracy: 0.6813 - val_loss: 1.0125\n",
            "Epoch 27/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.6686 - loss: 1.0716 - val_accuracy: 0.6856 - val_loss: 0.9960\n",
            "Epoch 28/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.6718 - loss: 1.0530 - val_accuracy: 0.6904 - val_loss: 0.9805\n",
            "Epoch 29/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.6783 - loss: 1.0390 - val_accuracy: 0.6944 - val_loss: 0.9660\n",
            "Epoch 30/100\n",
            "350/350 - 4s - 13ms/step - accuracy: 0.6804 - loss: 1.0217 - val_accuracy: 0.6974 - val_loss: 0.9522\n",
            "Epoch 31/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.6847 - loss: 1.0076 - val_accuracy: 0.7009 - val_loss: 0.9393\n",
            "Epoch 32/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.6894 - loss: 0.9921 - val_accuracy: 0.7046 - val_loss: 0.9271\n",
            "Epoch 33/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.6967 - loss: 0.9779 - val_accuracy: 0.7090 - val_loss: 0.9153\n",
            "Epoch 34/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.6959 - loss: 0.9688 - val_accuracy: 0.7121 - val_loss: 0.9045\n",
            "Epoch 35/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7012 - loss: 0.9544 - val_accuracy: 0.7161 - val_loss: 0.8940\n",
            "Epoch 36/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.7056 - loss: 0.9461 - val_accuracy: 0.7184 - val_loss: 0.8842\n",
            "Epoch 37/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.7101 - loss: 0.9323 - val_accuracy: 0.7217 - val_loss: 0.8749\n",
            "Epoch 38/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7132 - loss: 0.9231 - val_accuracy: 0.7253 - val_loss: 0.8659\n",
            "Epoch 39/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7136 - loss: 0.9143 - val_accuracy: 0.7276 - val_loss: 0.8575\n",
            "Epoch 40/100\n",
            "350/350 - 3s - 10ms/step - accuracy: 0.7173 - loss: 0.9039 - val_accuracy: 0.7304 - val_loss: 0.8493\n",
            "Epoch 41/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7193 - loss: 0.8960 - val_accuracy: 0.7355 - val_loss: 0.8413\n",
            "Epoch 42/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7225 - loss: 0.8863 - val_accuracy: 0.7371 - val_loss: 0.8340\n",
            "Epoch 43/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7232 - loss: 0.8792 - val_accuracy: 0.7398 - val_loss: 0.8267\n",
            "Epoch 44/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7287 - loss: 0.8713 - val_accuracy: 0.7426 - val_loss: 0.8199\n",
            "Epoch 45/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7296 - loss: 0.8648 - val_accuracy: 0.7445 - val_loss: 0.8134\n",
            "Epoch 46/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7313 - loss: 0.8580 - val_accuracy: 0.7478 - val_loss: 0.8070\n",
            "Epoch 47/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7348 - loss: 0.8479 - val_accuracy: 0.7502 - val_loss: 0.8010\n",
            "Epoch 48/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.7361 - loss: 0.8434 - val_accuracy: 0.7528 - val_loss: 0.7951\n",
            "Epoch 49/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.7410 - loss: 0.8347 - val_accuracy: 0.7553 - val_loss: 0.7895\n",
            "Epoch 50/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.7419 - loss: 0.8297 - val_accuracy: 0.7575 - val_loss: 0.7839\n",
            "Epoch 51/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.7453 - loss: 0.8259 - val_accuracy: 0.7607 - val_loss: 0.7786\n",
            "Epoch 52/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7459 - loss: 0.8194 - val_accuracy: 0.7629 - val_loss: 0.7736\n",
            "Epoch 53/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.7479 - loss: 0.8127 - val_accuracy: 0.7641 - val_loss: 0.7688\n",
            "Epoch 54/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7484 - loss: 0.8076 - val_accuracy: 0.7664 - val_loss: 0.7639\n",
            "Epoch 55/100\n",
            "350/350 - 6s - 16ms/step - accuracy: 0.7501 - loss: 0.8006 - val_accuracy: 0.7678 - val_loss: 0.7593\n",
            "Epoch 56/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.7532 - loss: 0.7959 - val_accuracy: 0.7691 - val_loss: 0.7548\n",
            "Epoch 57/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.7553 - loss: 0.7905 - val_accuracy: 0.7704 - val_loss: 0.7504\n",
            "Epoch 58/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.7554 - loss: 0.7877 - val_accuracy: 0.7713 - val_loss: 0.7463\n",
            "Epoch 59/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7589 - loss: 0.7825 - val_accuracy: 0.7721 - val_loss: 0.7422\n",
            "Epoch 60/100\n",
            "350/350 - 5s - 16ms/step - accuracy: 0.7599 - loss: 0.7785 - val_accuracy: 0.7735 - val_loss: 0.7383\n",
            "Epoch 61/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7622 - loss: 0.7717 - val_accuracy: 0.7745 - val_loss: 0.7346\n",
            "Epoch 62/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7622 - loss: 0.7696 - val_accuracy: 0.7763 - val_loss: 0.7307\n",
            "Epoch 63/100\n",
            "350/350 - 5s - 16ms/step - accuracy: 0.7619 - loss: 0.7663 - val_accuracy: 0.7771 - val_loss: 0.7271\n",
            "Epoch 64/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7652 - loss: 0.7594 - val_accuracy: 0.7786 - val_loss: 0.7235\n",
            "Epoch 65/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7661 - loss: 0.7575 - val_accuracy: 0.7798 - val_loss: 0.7201\n",
            "Epoch 66/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.7671 - loss: 0.7527 - val_accuracy: 0.7814 - val_loss: 0.7167\n",
            "Epoch 67/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7674 - loss: 0.7513 - val_accuracy: 0.7814 - val_loss: 0.7135\n",
            "Epoch 68/100\n",
            "350/350 - 5s - 13ms/step - accuracy: 0.7697 - loss: 0.7466 - val_accuracy: 0.7823 - val_loss: 0.7103\n",
            "Epoch 69/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.7719 - loss: 0.7431 - val_accuracy: 0.7837 - val_loss: 0.7073\n",
            "Epoch 70/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7712 - loss: 0.7401 - val_accuracy: 0.7849 - val_loss: 0.7041\n",
            "Epoch 71/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7741 - loss: 0.7369 - val_accuracy: 0.7862 - val_loss: 0.7010\n",
            "Epoch 72/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7727 - loss: 0.7345 - val_accuracy: 0.7870 - val_loss: 0.6981\n",
            "Epoch 73/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7757 - loss: 0.7308 - val_accuracy: 0.7879 - val_loss: 0.6954\n",
            "Epoch 74/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7767 - loss: 0.7250 - val_accuracy: 0.7888 - val_loss: 0.6927\n",
            "Epoch 75/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.7769 - loss: 0.7253 - val_accuracy: 0.7893 - val_loss: 0.6899\n",
            "Epoch 76/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.7813 - loss: 0.7199 - val_accuracy: 0.7897 - val_loss: 0.6872\n",
            "Epoch 77/100\n",
            "350/350 - 5s - 16ms/step - accuracy: 0.7788 - loss: 0.7185 - val_accuracy: 0.7906 - val_loss: 0.6846\n",
            "Epoch 78/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7806 - loss: 0.7144 - val_accuracy: 0.7921 - val_loss: 0.6821\n",
            "Epoch 79/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7805 - loss: 0.7125 - val_accuracy: 0.7917 - val_loss: 0.6798\n",
            "Epoch 80/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.7829 - loss: 0.7095 - val_accuracy: 0.7924 - val_loss: 0.6772\n",
            "Epoch 81/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7823 - loss: 0.7079 - val_accuracy: 0.7930 - val_loss: 0.6750\n",
            "Epoch 82/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7831 - loss: 0.7045 - val_accuracy: 0.7941 - val_loss: 0.6726\n",
            "Epoch 83/100\n",
            "350/350 - 5s - 16ms/step - accuracy: 0.7832 - loss: 0.7024 - val_accuracy: 0.7950 - val_loss: 0.6703\n",
            "Epoch 84/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7855 - loss: 0.6979 - val_accuracy: 0.7961 - val_loss: 0.6679\n",
            "Epoch 85/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7867 - loss: 0.6979 - val_accuracy: 0.7963 - val_loss: 0.6659\n",
            "Epoch 86/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7868 - loss: 0.6937 - val_accuracy: 0.7970 - val_loss: 0.6638\n",
            "Epoch 87/100\n",
            "350/350 - 3s - 10ms/step - accuracy: 0.7868 - loss: 0.6916 - val_accuracy: 0.7979 - val_loss: 0.6617\n",
            "Epoch 88/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7869 - loss: 0.6896 - val_accuracy: 0.7982 - val_loss: 0.6597\n",
            "Epoch 89/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7899 - loss: 0.6848 - val_accuracy: 0.7992 - val_loss: 0.6577\n",
            "Epoch 90/100\n",
            "350/350 - 3s - 9ms/step - accuracy: 0.7887 - loss: 0.6866 - val_accuracy: 0.7996 - val_loss: 0.6557\n",
            "Epoch 91/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.7883 - loss: 0.6831 - val_accuracy: 0.7996 - val_loss: 0.6538\n",
            "Epoch 92/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.7933 - loss: 0.6801 - val_accuracy: 0.7999 - val_loss: 0.6519\n",
            "Epoch 93/100\n",
            "350/350 - 6s - 17ms/step - accuracy: 0.7915 - loss: 0.6777 - val_accuracy: 0.8003 - val_loss: 0.6500\n",
            "Epoch 94/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.7922 - loss: 0.6776 - val_accuracy: 0.8012 - val_loss: 0.6481\n",
            "Epoch 95/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7925 - loss: 0.6759 - val_accuracy: 0.8011 - val_loss: 0.6464\n",
            "Epoch 96/100\n",
            "350/350 - 3s - 8ms/step - accuracy: 0.7931 - loss: 0.6731 - val_accuracy: 0.8019 - val_loss: 0.6448\n",
            "Epoch 97/100\n",
            "350/350 - 4s - 11ms/step - accuracy: 0.7946 - loss: 0.6696 - val_accuracy: 0.8025 - val_loss: 0.6429\n",
            "Epoch 98/100\n",
            "350/350 - 4s - 12ms/step - accuracy: 0.7934 - loss: 0.6695 - val_accuracy: 0.8032 - val_loss: 0.6414\n",
            "Epoch 99/100\n",
            "350/350 - 5s - 15ms/step - accuracy: 0.7950 - loss: 0.6668 - val_accuracy: 0.8032 - val_loss: 0.6398\n",
            "Epoch 100/100\n",
            "350/350 - 5s - 14ms/step - accuracy: 0.7954 - loss: 0.6653 - val_accuracy: 0.8040 - val_loss: 0.6380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Final Model & Test Set Evaluation\n",
        "\n",
        "## 5.3 Final Accuracy on Test Set (20%)\n",
        "\n",
        "Evaluate the final trained model on the test data and report the accuracy."
      ],
      "metadata": {
        "id": "SDyCAVpbC08H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the test data if it is not already flattened\n",
        "x_test_final_flat = x_test_final.reshape(x_test_final.shape[0], -1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = final_model.evaluate(x_test_final_flat, y_test_final_encoded, verbose=2)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9OJ6UJlC3FH",
        "outputId": "5cf71a19-682d-4837-89cc-9e17ba8cea18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "438/438 - 1s - 2ms/step - accuracy: 0.8078 - loss: 0.6296\n",
            "Test Loss: 0.6296\n",
            "Test Accuracy: 0.8078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conceptual Discussion\n",
        "\n",
        "## 6.1 Why Is It Harder to Train Deep Neural Networks?\n",
        "\n",
        "Training deep neural networks is challenging for several reasons. First, vanishing and exploding gradients occur when gradients become too small or too large during backpropagation, making it difficult for the network to learn effectively in early layers. Second, deeper networks have more parameters, which increases the complexity of the optimization process and can lead to slower or unstable training. Third, with more parameters, the risk of overfitting increases, as the model may memorize training data instead of generalizing well to new data. Finally, deeper networks demand more computational resources, leading to longer training times and the need for specialized hardware."
      ],
      "metadata": {
        "id": "UevHLjtaE60H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Conclusion & Observations\n",
        "\n",
        "In this study, we tuned several hyperparameters to improve the performance of our neural network model. Among the parameters, learning rate and the number of hidden units had the most significant impact on validation accuracy. Surprisingly, the baseline model performed just as well as the tuned model, achieving an average validation accuracy of 0.8845 across 4 folds, identical to the tuned model's accuracy.\n",
        "\n",
        "This outcome suggests that the baseline configuration was already well-optimized for the task, and further tuning did not yield noticeable improvements. Additionally, the test evaluation resulted in a loss of 0.6296 and an accuracy of 0.8078, indicating reasonable generalization but also room for improvement in model robustness or architecture."
      ],
      "metadata": {
        "id": "oEy9eZXxE98c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}